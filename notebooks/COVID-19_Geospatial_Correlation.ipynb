{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "from timeit import default_timer as timer\n",
    "import os, io, re, pandas as pd, numpy as np\n",
    "import logging\n",
    "from ibmpairs import paw\n",
    "import scipy.stats\n",
    "from scipy.stats import spearmanr\n",
    "import requests\n",
    "import json\n",
    "import yaml\n",
    "import geopandas as gpd\n",
    "from itertools import cycle\n",
    "from IPython.core.display import HTML\n",
    "# library used for visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "# package supporting GAM implementation in R\n",
    "import packages.gaa.gam as gamModel\n",
    "import packages.gaa.analysis as analysisHelper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Geospatial Correlation & Association\n",
    "This Notebook can be used to determine the [**Spearman's rank correlation coefficient**](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) or a [**log-linear generalized additive model (GAM)**](https://en.wikipedia.org/wiki/Generalized_additive_model) between COVID-19 cases and Geospatial & Temporal information out of [**IBM PAIRS Geoscope**](https://ibmpairs.mybluemix.net/).\n",
    "\n",
    "The Notebook will run through 10 steps.\n",
    "\n",
    "If you just want to run the Notebook you need to provide input for **Step 1** & **Step 2**. You can then run the analysis for the included countries which are **The Netherlands (NL), France (FR), Denmark (DK), Sweden (SE), Germany (DE), United States (US)** & **India (IN)**. Steps 3 up to 10 will run without additional input, but can be altered where necessary.\n",
    "\n",
    "If you want to extend the Notebook with additional countries you need to extend **Step 4** & **Step 5** following the examples of the already included countries.\n",
    "\n",
    "Please note that for DK & SE you need to download the COVID-19 files manually. The sources are specified in **Step 4**. The other countries download the COVID-19 data directly from a URL.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Create User Config File**: an IBM PAIRS User Account with API access is needed. These credentials must be stored in a private configuration file.\n",
    "\n",
    " NB: If no account is available the Notebook can still be used. For this, do not create the config file as defined in Step 1. The Notebook will continue but only with IBM PAIRS Local Cached data. As example a cached NL dataset for TemperatureAboveGround, RelativeHumiditySurface, WindSpeed, UVIndex is included.\n",
    " \n",
    "2. **Define Analysis**: the Notebook must be configured to run the analysis for the desired Country, Time & Variables\n",
    "\n",
    "3. **Set Global Variables**: set global variables that are used in the remainder of the Notebook\n",
    "\n",
    "4. **Get Country Geospatial Data**: collect metadata of the country (e.g. regions & population) and geospatial vector data\n",
    "\n",
    "5. **Get Country COVID-19 Data Set**: collect COVID-19 case data (e.g. hospitalized, recovered, deceased patients)\n",
    "\n",
    "6. **Get IBM PAIRS Geospatial & Temporal data**: collect the geospatial information needed for the analysis\n",
    "\n",
    "7. **Add Rolling Windows & Time Shifts**: add rolling windows & time shifts to model e.g. incubation time\n",
    "8. **Merge COVID-19 & IBM PAIRS datasets**: merge the data on COVID-19 with the geospatial dataset\n",
    "9. **Determine Spearman or GAM**: calculate the spearman correlation coefficient or GAM association and the significance of it\n",
    "10. **Create Visualizations**: create various plots (e.g. line charts, SPLOMs, Choropleths) to visualize the results of the input & analysis result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create User Config File\n",
    "To use this Notebook you need to create a YAML file **private-user-config.yml** in the root the folder. This file holds the access credentials to IBM PAIRS. The YAML file needs the following structure:\n",
    "```\n",
    "    ibm_pairs:\n",
    "        server: https://pairs.res.ibm.com\n",
    "        user:\n",
    "        api_key: \n",
    "```\n",
    "If you don't have access to the IBM PAIRS API, then you can request accesss via the [IBM PAIRS Access procedure](../IBMPAIRS-Access.md).\n",
    "\n",
    "If you want to use the Notebook without access to IBM PAIRS you can. However, you are restricted to the IBM PAIRS Cache File that is included in this GitHub Repo for NL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FILE_PATH          = os.path.realpath(\"../\")\n",
    "USER_CONFIG             = None\n",
    "USER_CONFIG_FILE        = 'private-user-config.yml'\n",
    "\n",
    "try:\n",
    "    with open(BASE_FILE_PATH + \"/\" + USER_CONFIG_FILE, 'r') as user_config_file:\n",
    "        USER_CONFIG = yaml.safe_load(user_config_file)\n",
    "except:\n",
    "    print('Cannot find {}/{}'.format(BASE_FILE_PATH,USER_CONFIG_FILE))\n",
    "    print('\\nPlease create this file or continue while using cached IBM PAIRS only.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Analysis\n",
    "The variable **ANALYSIS** controls what analysis is executed by running the Notebook. The 'analysis' key must be set to a value that is present in the **ANALYSIS_DEFINITIONS** dictionary.\n",
    "\n",
    "Each **ANALYSIS_DEFINITION** must have a unique key and contains 3 values:\n",
    "- *country_code*: ISO-3166 of the country for which the analysis is run. Seven countries are made available in this notebook (NL, FR, DK, SE, DE, US, IN) but more can be added.\n",
    "\n",
    "    The country_code determines:\n",
    "    1. the geospatial boundries\n",
    "    2. the COVID-19 datasource\n",
    "    3. the data queried from IBM PAIRS\n",
    "    \n",
    "    \n",
    "- *model*: MODEL_DEFINITIONS key, used to define the algorithm & parameters used for the analysis\n",
    "- *time_window*: TIME_WINDOW_DEFINITIONS key, used to resrict the analysis to a specific time slice\n",
    "\n",
    "Each **MODEL_DEFINITION** must have a unique key and contains the parameters appropriate for the given model. The current Notebook supports *spearman* and *gam*, additional models might be added in later versions.\n",
    "\n",
    "The **spearman** model has 6 values:\n",
    "- *model*: controls the algorithm to use, set to 'spearman' to run a spearman correlaction model.\n",
    "- *predictor*: first rank variable in the spearman algorithm. Must be an existing key in PAIRS_QUERY_DEFINITONS. Only the first predictor specified is currently used in the analysis & visualization.\n",
    "- *outcome*: second rank variable(s) in the spearman algorithm. The correlation between the *predictor* and all *outcome* variables specified is determined. The *outcome* variables refer to the columnns available in the cleansed data source. Regular expressions can be used to specify e.g. wildcards.\n",
    "- *alpha*: threshold to determine whether the correlation is deemed significant.\n",
    "- *rolling_windows*: the number of days for which the rolling window is calculated. Multiple windows can be specified.\n",
    "- *rolling_window_type*: the arithmetic operation applied for the rolling_window. Supported are 'mean' and 'sum'\n",
    "- *time_shifts*: the number of days the predictor is shifted. The time shift can be used to model the impact of a time lag between the predictor and the outcome. Multiple time shifts can be specified.\n",
    "\n",
    "The **gam** model has 7 values:\n",
    "- *model*: controls the algorithm to use, set to 'gam' to run a log-linear GAM.\n",
    "- *independent_variables*: predictor variables for which it is determined if there is an association with the dependent variable(s) specified in the *outcome* field. Must be an existing key in PAIRS_QUERY_DEFINITONS.\n",
    "- *control_variables*: [confounding variables](https://en.wikipedia.org/wiki/Confounding) which are variables, other than the independent variables in focus, that may affect the outcome and thus, may lead to erroneous conclusions about the relationship between the independent and outcome variables. Must be an existing key in PAIRS_QUERY_DEFINITONS and / or DOW (Day of Week).\n",
    "- *outcome*: dependent variable(s) in the GAM algorithm. The association between the *independent_variables* and all *outcome* variables specified is determined. The *outcome* variables refer to the columnns available in the cleansed data source. Regular expressions can be used to specify e.g. wildcards.\n",
    "- *alpha*: threshold to determine whether the association is deemed significant.\n",
    "- *rolling_windows*: the number of days for which the rolling window is calculated. Multiple windows can be specified.\n",
    "- *rolling_window_type*: the arithmetic operation applied for the rolling_window. Supported are 'mean' and 'sum'\n",
    "- *time_shifts*: the number of days the predictor is shifted. The time shift can be used to model the impact of a time lag between the predictor and the outcome. Multiple time shifts can be specified.\n",
    "\n",
    "Each **TIME_WINDOW_DEFINITION** must have a unique key and controls the time window for which the analysis is run. Each definition has 2 values:\n",
    "\n",
    "- *window_start*: the first date to include in the analysis.\n",
    "- *window_end*: the last date to include in the analysis.\n",
    "\n",
    "A date can be fixed (e.g. date(2020, 3, 6)) or relative (e.g. date.today()) as long as it is a valid [Python datetime.date](https://docs.python.org/3.8/library/datetime.html).\n",
    "\n",
    "NB: The cases in a given country are presumably heavily influenced by the time of introduction of the virus in the region and the measures taken to control the outbreak. Therefore one must give a good consideration on the influence of the Time Window on the observered correlation.\n",
    "\n",
    "As **example only** the Notebook contains a definition for each region. The examples are based on the [Oxford COVID-19 Government Response Tracker](https://covidtracker.bsg.ox.ac.uk/). The *start_date* is set to the first day with 100+ cases and the *end_date* is set to 14 days after the Stringency Index is 70+ for the given country.   \n",
    "\n",
    "Each **PAIRS_QUERY_DEFINITION** must have a unique key and controls what data is queried from IBM PAIRS. The *predictor* value(s) determine the definition used in the given model. Each definition has 2 values:\n",
    "\n",
    "- *layer_id*: ID of the layer in the IBM PAIRS Dataset that is used in the model (e.g. 49311 for the *UV Index* layer out of the *Current and historical weather (IBM TWC)* dataset)\n",
    "- *aggregation*: the arithmetic operation applied for the temporal aggregation if there are multiple measurements for the same raster in IBM PAIRS. Supported are 'None', 'Min', 'Max', 'Mean', 'Sum'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis to run in the Notebook\n",
    "ANALYSIS                = {'analysis':'GAM_ARTICLE_4_PLOT_1'}\n",
    "\n",
    "ANALYSIS_DEFINITIONS    = {\n",
    "    'ARTICLE_2_PLOT_1' : {\n",
    "        'country_code':'NL',\n",
    "        'model':       'SpearmanUVIndex_AllOutcomeAvailable',\n",
    "        'time_window': 'ARTICLE_2_PLOT_1'},\n",
    "    'ARTICLE_2_PLOT_2' : {\n",
    "        'country_code':'NL','model':'SpearmanUVIndex_AllOutcomeAvailable','time_window':'ARTICLE_2_PLOT_2'},\n",
    "    'NL_UVIndex_ToLockdown' : {\n",
    "        'country_code':'NL','model':'SpearmanUVIndex_AllOutcomeAvailable','time_window':'NL_Start->LockDown+14d'},\n",
    "    'FR_UVIndex_ToLockdown' : {\n",
    "        'country_code':'FR','model':'SpearmanUVIndex_AllOutcomeAvailable','time_window':'FR_Start->LockDown+14d'},\n",
    "    'DK_UVIndex_ToLockdown' : {\n",
    "        'country_code':'DK','model':'SpearmanUVIndex_AllOutcomeAvailable','time_window':'DK_Start->LockDown+14d'},\n",
    "    'SE_UVIndex_Today': {\n",
    "        'country_code':'SE','model':'SpearmanUVIndex_AllOutcomeAvailable','time_window':'SE_Start->Today'},\n",
    "    'DE_UVIndex_ToLockdown' : {\n",
    "        'country_code':'DE','model':'SpearmanUVIndex_AllOutcomeAvailable','time_window':'DE_Start->LockDown+14d'},   \n",
    "    'US_UVIndex_Today' : {\n",
    "        'country_code':'US','model':'SpearmanUVIndex_AllOutcomeAvailable','time_window':'US_Start->Today'},\n",
    "    'IN_UVIndex_Today' : {\n",
    "        'country_code':'IN','model':'SpearmanUVIndex_AllOutcomeAvailable','time_window':'WHO_PandemicStart->Today'},\n",
    "    'Spearman_ARTICLE_4_PLOT_1' : {\n",
    "        'country_code':'NL',\n",
    "        'model':       'SpearmanTemp_LimitOutcome',\n",
    "        'time_window': 'ARTICLE_4_PLOT_1'},\n",
    "    'GAM_ARTICLE_4_PLOT_1' : {\n",
    "        'country_code':'NL',\n",
    "        'model':       'Gam_UTR_WD_LimitOutcome',\n",
    "        'time_window': 'ARTICLE_4_PLOT_1'},\n",
    "}\n",
    "\n",
    "MODEL_DEFINITIONS    = {\n",
    "    'SpearmanUVIndex_LimitOutcome' : {\n",
    "        'model':'spearman',\n",
    "        'predictor':['UVIndex'],\n",
    "        'outcome': ['hospitalized_addition_population_weighted', 'deceased_addition_population_weighted'],\n",
    "        'alpha': 0.001,\n",
    "        'rolling_windows':[7],\n",
    "        'rolling_window_type':'mean', \n",
    "        'time_shifts' : [0,7,14]\n",
    "        },\n",
    "    \n",
    "    'SpearmanUVIndex_AllOutcomeAvailable' : {\n",
    "        'model':'spearman',\n",
    "        'predictor':['UVIndex'],\n",
    "        'outcome': ['.+_addition_population_weighted'],\n",
    "        'alpha': 0.001,\n",
    "        'rolling_windows':[7],\n",
    "        'rolling_window_type':'sum', \n",
    "        'time_shifts' : [0,7,14]\n",
    "        },\n",
    "    \n",
    "    'SpearmanRelativeHumiditySurface_LimitOutcome' : {\n",
    "        'model':'spearman',\n",
    "        'predictor':['RelativeHumiditySurface'],\n",
    "        'outcome': ['hospitalized_addition_population_weighted', 'deceased_addition_population_weighted'],\n",
    "        'alpha': 0.001,   \n",
    "        'rolling_windows':[7],\n",
    "        'rolling_window_type':'mean', \n",
    "        'time_shifts' : [0,7,14]\n",
    "        },\n",
    "\n",
    "    'SpearmanTemp_LimitOutcome' : {\n",
    "        'model':'spearman',\n",
    "        'predictor':['UVIndex'],\n",
    "        'outcome': ['confirmed_addition_population_weighted'],\n",
    "        'alpha': 0.05,   \n",
    "        'rolling_windows':[1,7],\n",
    "        'rolling_window_type':'mean',\n",
    "        'time_shifts' : [0,7,14]\n",
    "        }, \n",
    "\n",
    "    'Gam_UTR_WD_LimitOutcome' : {\n",
    "        'model':'gam',\n",
    "        'independent_variables':['UVIndex', 'TemperatureAboveGround', 'RelativeHumiditySurface'],\n",
    "        'control_variables':['WindSpeed','DOW'],\n",
    "        'outcome': ['confirmed_addition'],\n",
    "        'alpha': 0.05,   \n",
    "        'rolling_windows':[1,7],\n",
    "        'rolling_window_type':'mean',\n",
    "        'time_shifts' : [0,7,14]\n",
    "        }, \n",
    "}\n",
    "\n",
    "TIME_WINDOW_DEFINITIONS    = {\n",
    "    'ARTICLE_2_PLOT_1': {'window_start' : date(2020, 3, 6), 'window_end' : date(2020, 6, 19)},\n",
    "    'ARTICLE_2_PLOT_2': {'window_start' : date(2020, 4, 1), 'window_end' : date(2020, 6, 19)},\n",
    "    'ARTICLE_4_PLOT_1': {'window_start' : date(2020, 4, 1), 'window_end' : date(2020, 11, 15)},\n",
    "    'NL_SecondWave': {'window_start' : date(2020, 8, 31), 'window_end' : date(2020, 11, 15)},\n",
    "    'WHO_PandemicStart->Today' :{'window_start' : date(2020, 3, 11), 'window_end' : date.today()},\n",
    "    'NL_Start->LockDown+14d' :{'window_start' : date(2020, 3, 6), 'window_end' : date(2020, 3, 30)},\n",
    "    'FR_Start->LockDown+14d' :{'window_start' : date(2020, 3, 1), 'window_end' : date(2020, 5, 1)},\n",
    "    'DK_Start->LockDown+14d' :{'window_start' : date(2020, 3, 10), 'window_end' : date(2020, 3, 27)},\n",
    "    'SE_Start->Today' :{'window_start' : date(2020, 3, 7), 'window_end' : date.today()},\n",
    "    'DE_Start->LockDown+14d' :{'window_start' : date(2020, 3, 1), 'window_end' : date(2020, 4, 4)},\n",
    "    'US_Start->LockDown+14d' :{'window_start' : date(2020, 3, 3), 'window_end' : date(2020, 4, 2)},\n",
    "    'IN_Start->LockDown+14d' :{'window_start' : date(2020, 3, 16), 'window_end' : date(2020, 4, 3)},\n",
    "}\n",
    "\n",
    "# parameters used in IBM PAIRS Query\n",
    "PAIRS_QUERY_DEFINITIONS = {\n",
    "    'TemperatureAboveGround': {'layer_id':'49257', 'aggregation':'Mean'},\n",
    "    'RelativeHumiditySurface': {'layer_id':'49252', 'aggregation':'Mean'},\n",
    "    'WindSpeed': {'layer_id':'49313', 'aggregation':'Mean'},\n",
    "    'UVIndex': {'layer_id':'49311', 'aggregation':'Sum'},\n",
    "    'SolarRadiation': {'layer_id':'49424', 'aggregation':'Sum'},\n",
    "    'Soiltemperature': {'layer_id':'49446', 'aggregation':'Sum'},\n",
    "    'Surfacepressure': {'layer_id':'49439', 'aggregation':'Sum'},\n",
    "    'Totalprecipitation': {'layer_id':'49459', 'aggregation':'Sum'},\n",
    "    'Dewpoint': {'layer_id':'49422', 'aggregation':'Sum'},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Global Variables\n",
    "In order to simplify the remainer of the Notebook a series of Global Variables are set. The coding convention in the Notebook is that Global Variables are defined in CAPITALS.\n",
    "\n",
    "The Global Variables should **not be changed**, unless you want to alter the code in the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the analysis the Notebook will process\n",
    "ANALYSIS_DEFINITION     = ANALYSIS_DEFINITIONS[ANALYSIS['analysis']]\n",
    "ANALYSIS['country_code']= ANALYSIS_DEFINITION['country_code']\n",
    "ANALYSIS['model']       = MODEL_DEFINITIONS[ANALYSIS_DEFINITION['model']]\n",
    "ANALYSIS['time_window'] = TIME_WINDOW_DEFINITIONS[ANALYSIS_DEFINITION['time_window']]\n",
    "\n",
    "# Define the predictors in case of a gam model, this model has two types of variables of which some are not retrieved from PAIRS\n",
    "if(ANALYSIS['model']['model'] == 'gam'):\n",
    "    ANALYSIS['model']['predictor'] = ANALYSIS['model']['independent_variables'] + ANALYSIS['model']['control_variables']\n",
    "    for predictor in ANALYSIS['model']['predictor']:\n",
    "        if (predictor not in PAIRS_QUERY_DEFINITIONS.keys()):\n",
    "            ANALYSIS['model']['predictor'].remove(predictor)\n",
    "        \n",
    "# Define the paths to locally store & retrieve files\n",
    "GLOBAL_FILE_PATH        = BASE_FILE_PATH + '/data/Global/'\n",
    "COUNTRY_FILE_PATH       = BASE_FILE_PATH + '/data/' + ANALYSIS_DEFINITION['country_code'] + '/'\n",
    "\n",
    "# Add the file path to the ANALYSIS definiton so we can cache the file\n",
    "ANALYSIS['country_file_path'] = COUNTRY_FILE_PATH\n",
    "ANALYSIS['cache_file']  = '{}IBMPAIRS_ANALYSIS_{}.csv'.format(COUNTRY_FILE_PATH,ANALYSIS['analysis'])\n",
    "\n",
    "# Set the values to access IBM PAIRS\n",
    "PAIRS_CACHE_REFRESH     = not(USER_CONFIG == None) # Used to control if only the local cache file is used\n",
    "if(PAIRS_CACHE_REFRESH):\n",
    "    PAIRS_SERVER        = USER_CONFIG['ibm_pairs']['server']\n",
    "    PAIRS_CREDENTIALS   = (USER_CONFIG['ibm_pairs']['user'], USER_CONFIG['ibm_pairs']['api_key'])\n",
    "\n",
    "# Define the PAIRS_QUERY to run    \n",
    "PAIRS_QUERY             = {}\n",
    "PAIRS_QUERY['alias']    = '_'.join(ANALYSIS['model']['predictor'])\n",
    "PAIRS_QUERY['cache_file'] = '{}IBMPAIRS_{}.csv'.format(COUNTRY_FILE_PATH,'LocalCache')\n",
    "PAIRS_QUERY['layers']   = {}\n",
    "\n",
    "additional_days_for_layer = []\n",
    "ANALYSIS['pairs_query'] = {}\n",
    "ANALYSIS['pairs_query']['layers'] = {}\n",
    "for predictor in ANALYSIS['model']['predictor']: \n",
    "    PAIRS_QUERY['layers'][predictor] = PAIRS_QUERY_DEFINITIONS[predictor]\n",
    "    ANALYSIS['pairs_query']['layers'][predictor] = PAIRS_QUERY['layers'][predictor]\n",
    "    additional_days_for_layer.append(\\\n",
    "        max(ANALYSIS['model']['rolling_windows']) +\\\n",
    "        max(ANALYSIS['model']['time_shifts']))\n",
    "\n",
    "# Define the start & end date for the data we need from IBM PAIRS where we take into account:\n",
    "#   - the extra days we need before the window_start to accomodate the rolling_windows\n",
    "#   - the extra days we need before the window_start to accomodate the time_shifts\n",
    "PAIRS_QUERY['start_date'] = ANALYSIS['time_window']['window_start'] \\\n",
    "                                - timedelta(days=(max(additional_days_for_layer) + 1))\n",
    "PAIRS_QUERY['end_date'] = ANALYSIS['time_window']['window_end'] \\\n",
    "                                if ANALYSIS['time_window']['window_end'] < date.today() \\\n",
    "                                else date.today() - timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAIRS_CACHE_REFRESH = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get Country Geospatial Data\n",
    "To define a (new) country in the Notebook, create a **country_metadata.json** JSON file and select the **geometry** out of a shapefile.\n",
    "\n",
    "1. Create a **subfolder** in the *data* folder using the [*ISO 3166-1 alpha-2*](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2) code for the given country: \n",
    "2. Create a **country_metadata.json** in that country folder with one top level object *country_metadata* and for each region create one entry as follows:\n",
    "\n",
    "    \"country_metadata\": [\n",
    "        {\n",
    "            \"iso3166-1_code\": \"NL\",\n",
    "            \"iso3166-1_name\": \"The Netherlands\",\n",
    "            \"iso3166-2_code\": \"NL-UT\",\n",
    "            \"iso3166-2_name_en\": \"Utrecht\",\n",
    "            \"population\": 1354979,\n",
    "            \"covid_region_code\": \"26\"\n",
    "        },]\n",
    "Each region is identified by an [ISO 3166-2](https://en.wikipedia.org/wiki/ISO_3166-2) code.\n",
    "    - *covid_region_code*: unique identifier of a region as used by the COVID-19 data source. Sometimes these sources use ISO 3166-2 as well, but often another unique reference is used. Used to establish the merge between the country geospatial data and the COVID-19 data.\n",
    "    - *population*: total amount of people in the region. Used to determine the *population weighted* COVID-19 metric.\n",
    "\n",
    " Add a second top level object *country_metadata_sources* in to the JSON file to document the sources used and capture comments if any:\n",
    "\n",
    "    \"country_data_sources\": {\n",
    "        \"iso3166-1\":\"https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2\",\n",
    "        \"iso3166-2\":\"https://en.wikipedia.org/wiki/ISO_3166-2\",\n",
    "        \"population\":\"https://opendata.cbs.nl/statline/#/CBS/nl/dataset/70072NED/table?fromstatweb\"\n",
    "    }\n",
    "\n",
    "\n",
    "3. Extend the **get_country_region_geometry()** function with a new function that filters the geospatial vector data from the shapefile. The geospatial data used in this Notebook is the [*Admin 1 – States, Provinces*](https://www.naturalearthdata.com/downloads/10m-cultural-vectors/) shapefile.\n",
    "\n",
    " Possibly the data must be filtered and / or cleansed to arrive at the clean data set for the country. The outcome must be a Geopandas Data frame with the following columns:\n",
    "\n",
    "    - *iso3166-2_code*: unique identified for the region\n",
    "    - *geometry:* Polygon or Multi Polygon defining the boundaries of the region\n",
    "    - *geometry_wkt:* geometry information in Well-Known text (WKT) format (the format used in PAIRS)\n",
    "\n",
    " NB: If you use another geospatial source, make sure the spatial reference system is WGS 84 (or EPSG:4326). This is required bij IBM PAIRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the files that hold the country metadata & shapefile data\n",
    "COUNTRY_METADATA_JSON   = COUNTRY_FILE_PATH + 'country_metadata.json'\n",
    "SHAPEFILE_FILE_PATH = GLOBAL_FILE_PATH + 'ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shp'\n",
    "\n",
    "# Function that loads the country_metadata JSON file & collects the geospatial vector data\n",
    "def get_country_geospatial_data(country_code, country_metadata_file, shapefile):\n",
    "    # load the JSON file that contains the meta data for the country\n",
    "    try:\n",
    "        with open(country_metadata_file, 'r', encoding='utf-8') as metadata_file:\n",
    "            country_metadata=json.load(metadata_file)\n",
    "    except:\n",
    "        raise Exception('ERROR: Cannot find {}. Please create this file to continue.'.format(country_metadata_file))\n",
    "        \n",
    "    country_metadata_df = pd.json_normalize(country_metadata['country_metadata'])\n",
    "\n",
    "    # load the country geometry vector data & merge with the country_metadata\n",
    "    try:\n",
    "        geometry_json, country_metadata_df = get_country_geometry(country_code, country_metadata_df, shapefile)\n",
    "    except Exception as inst:\n",
    "        raise Exception('ERROR: {}. Please define a country_geometry function to continue.'.format(inst))\n",
    "\n",
    "    print(\"Country Geospatial Data loaded. There are {} regions defined.\\n\".format(country_metadata_df.shape[0]))\n",
    "    print(\"The following sources have been used to contruct the country geospatial data:\".format())\n",
    "\n",
    "    for key,value in country_metadata['country_data_sources'].items():\n",
    "        print(\" - {}: {}\".format(key, value))        \n",
    "    \n",
    "    return geometry_json, country_metadata_df\n",
    "\n",
    "# Function that filters the geospatial vector data from the shapefile\n",
    "# Extend when new ANALYSIS_DEFINITION['country_code']\n",
    "def get_country_geometry(country_code, country_metadata_df, shapefile):\n",
    "    #Read the (uncleansed) shapefile\n",
    "    shapefile_gdp = gpd.read_file(shapefile)\n",
    "\n",
    "    #Cleanse the data for Country specific updates to arrive at a standard gdp with\n",
    "    if(country_code == 'NL'):\n",
    "        country_region_gdp = get_country_geometry_basic_filter(shapefile_gdp,country_code,'Province')\n",
    "    elif(country_code == 'FR'):\n",
    "        country_region_gdp = get_FR_geometry_basic_filter(country_metadata_df,shapefile_gdp,country_code,'Metropolitan department')\n",
    "    elif(country_code == 'DK'):\n",
    "        country_region_gdp = get_country_geometry_basic_filter(shapefile_gdp,country_code,'Region')\n",
    "    elif(country_code == 'SE'):\n",
    "        country_region_gdp = get_country_geometry_basic_filter(shapefile_gdp,country_code,'County')\n",
    "    elif(country_code == 'DE'):\n",
    "        country_region_gdp = get_country_geometry_basic_filter(shapefile_gdp,country_code,'State')\n",
    "    elif(country_code == 'US'):\n",
    "        states_to_exclude = ['US-AK','US-HI']\n",
    "        country_region_gdp = get_country_geometry_basic_filter(shapefile_gdp,country_code,'State',states_to_exclude)\n",
    "    elif(country_code == 'IN'):\n",
    "        country_region_gdp = get_country_geometry_basic_filter(shapefile_gdp,country_code)\n",
    "    else:\n",
    "        raise Exception('Geometry not implemented for country_code: ' + country_code)\n",
    "    \n",
    "    #Convert to GeoJSON\n",
    "    country_region_json = json.loads(country_region_gdp.to_json())\n",
    "    \n",
    "    #Add the geometry information to the country metadata\n",
    "    country_metadata_df = country_metadata_df.merge(country_region_gdp,\n",
    "                                                   left_on='iso3166-2_code', right_on='iso3166-2_code', how='inner')\n",
    "\n",
    "    #Store the geometry value in WKT format as well since this is used for PAIRS\n",
    "    country_metadata_df['geometry_wkt'] = country_metadata_df['geometry'].apply(lambda g: g.wkt)\n",
    "\n",
    "    return country_region_json, country_metadata_df\n",
    "\n",
    "# Standard function that can be used by most countries to filter the vector data from the shapefile\n",
    "def get_country_geometry_basic_filter(shapefile_gdp, iso_filter, \n",
    "                                      type_filter = None, iso3166_2_code_exclusion_filter = None):\n",
    "    if(type_filter == None):\n",
    "        country_region_gdp = shapefile_gdp[(shapefile_gdp['iso_a2'] == iso_filter)]\n",
    "    else:\n",
    "        country_region_gdp = shapefile_gdp[(shapefile_gdp['iso_a2'] == iso_filter) & (shapefile_gdp['type_en'] == type_filter)]\n",
    "#   country_region_gdp.to_csv('country_region_gdp.csv', index=False)\n",
    "    country_region_gdp = country_region_gdp[['iso_3166_2', 'geometry']]\n",
    "    country_region_gdp.columns = ['iso3166-2_code', 'geometry']\n",
    "    \n",
    "    if not(iso3166_2_code_exclusion_filter == None):\n",
    "        country_region_gdp = country_region_gdp[~country_region_gdp['iso3166-2_code'].isin(iso3166_2_code_exclusion_filter)]\n",
    "    \n",
    "    return country_region_gdp\n",
    "\n",
    "# FR departments where reorganized in 2016, but the shapefile is using the old departments\n",
    "# FR specific filtering and data cleansing is needed to align the geospatial information to current regions \n",
    "def get_FR_geometry_basic_filter(country_metadata_df, shapefile_gdp, iso_filter, type_filter):\n",
    "    country_region_gdp = shapefile_gdp[(shapefile_gdp['iso_a2'] == iso_filter) & (shapefile_gdp['type_en'] == type_filter)]\n",
    "    country_region_gdp = country_region_gdp[['region_cod', 'geometry']]\n",
    "\n",
    "    # The metadata file for FR contains a specific fied\n",
    "    fr_region_map = pd.DataFrame(columns = ['region_cod', 'iso3166-2_code'])\n",
    "    for index, region in country_metadata_df.iterrows():\n",
    "        for reg_code in region['FR_nashapefile_old_region_code'].split(','):\n",
    "            fr_region_map = fr_region_map.append({'region_cod' : reg_code , 'iso3166-2_code' : region['iso3166-2_code']} , ignore_index=True)\n",
    "\n",
    "    # The mapping we use to add the iso3166-2_code to the shapefile\n",
    "    country_region_gdp = shapefile_gdp.merge(fr_region_map, \n",
    "                                                         left_on='region_cod', right_on='region_cod', how='inner')\n",
    "\n",
    "    # We merge the geometry information so that we end up with the 13 regions\n",
    "    country_region_gdp = country_region_gdp[['iso3166-2_code','geometry']]\n",
    "    country_region_gdp = country_region_gdp.dissolve(by='iso3166-2_code')\n",
    "    country_region_gdp = country_region_gdp.reset_index()\n",
    "    return country_region_gdp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_region_choropleth(country_code, country_region_json, country_metadata_df):\n",
    "    fig = px.choropleth(title='<b>Country Geospatial Selection & Population: {}</b>'.format(country_code),\n",
    "                        geojson=country_region_json, featureidkey=\"properties.iso3166-2_code\", \n",
    "                        data_frame=country_metadata_df,locations='iso3166-2_code',\n",
    "                        color='population',\n",
    "                        color_continuous_scale = 'Blues')\n",
    "    fig.update_geos(fitbounds=\"locations\", projection={'type':'mercator'}, visible=False)\n",
    "    fig.update_layout(\n",
    "        margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0}, height=400,\n",
    "        yaxis=dict(position=0),\n",
    "        font_size=8,title=dict(x=0,font_size=16),\n",
    "        legend=dict(title_font_size=10, font_size=8),        \n",
    "        coloraxis_colorbar=dict(title=\"Population\",lenmode=\"pixels\", len=300))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get the Country Metadata & Country Geospatial Vector Data\n",
    "    COUNTRY_REGION_JSON, COUNTRY_METADATA_DF = \\\n",
    "        get_country_geospatial_data(ANALYSIS['country_code'], COUNTRY_METADATA_JSON, SHAPEFILE_FILE_PATH)\n",
    "\n",
    "    # Create a Choropleth to visualize the Country & Regions being analysed    \n",
    "    create_region_choropleth(ANALYSIS['country_code'], COUNTRY_REGION_JSON, COUNTRY_METADATA_DF)\n",
    "except Exception as inst:\n",
    "    print(inst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Get Country COVID-19 Data Set\n",
    "The Country Specific COVID-19 data set is retrieved. Since available metrics differ widely between countries in terms of terminology, granularity and definition they need to be harmonized before standardized analysis can be run.\n",
    "\n",
    "Ideally the following **metrics** are obtained on a regional level:\n",
    "- **confirmed**: individual tested positive for COVID-19\n",
    "- **hospitalized**: individual admitted to a general hospital and tested positive for COVID-19\n",
    "- **hospitalized_icu**: individual admitted to a ICU unit in the hospital and tested positive for COVID-19\n",
    "- **recovered**: individual confirmed to have recovered from COVID-19\n",
    "- **deceased**: individual confirmed to have passed away with COVID-19 infection\n",
    "\n",
    "Ideally for each of the metrics the following values are listed per day:\n",
    "- **{metric}_addition**: increase of that given day\n",
    "- **{metric}_subtraction**: decline of that given day\n",
    "- **{metric}_absolute**: running sum of additions minus the running sum of subtractions\n",
    "- **{metric}_cumulative**: total amount up to that day (running sum of additions)\n",
    "\n",
    "If _cumulative metrics are provided, without corresponding _addition metric, then the _addition metric is derived from the _cumulative metric.\n",
    "\n",
    "For each of the available metrics a **_population_weighted** value is added by taking the original input from the source, divided by the *population* as defined in the country metadata.\n",
    "\n",
    "If a new country is added the **get_country_covid_data** must be modified to add the country. Data retrieval and cleasing will be country specific bu the net result of the code must be:\n",
    "1. The COVID-19 source is obtained, cleansed and loaded into a Data Frame\n",
    "2. The DF has a 'date' and 'region_code' column for correlation & mapping and matches the covid_region_code as defined in the country metadata\n",
    "3. The DF has standardized metric columns per the definition above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the country specific covid data and change it into a standard format\n",
    "def get_country_covid_data(analysis):\n",
    "    country_code = analysis['country_code']\n",
    "    try:\n",
    "        if(country_code == 'NL'):\n",
    "            covid_source_df_cleansed = get_NL_covid_data()\n",
    "        elif(country_code == 'FR'):\n",
    "            covid_source_df_cleansed = get_FR_covid_data()\n",
    "        elif(country_code == 'DK'):\n",
    "            covid_source_df_cleansed = get_DK_covid_data(analysis['country_file_path'])\n",
    "        elif(country_code == 'SE'):\n",
    "            covid_source_df_cleansed = get_SE_covid_data(analysis['country_file_path'])\n",
    "        elif(country_code == 'DE'):\n",
    "            covid_source_df_cleansed = get_DE_covid_data()\n",
    "        elif(country_code == 'US'):\n",
    "            covid_source_df_cleansed = get_US_covid_data()\n",
    "        elif(country_code == 'IN'):\n",
    "            covid_source_df_cleansed = get_IN_covid_data()\n",
    "        else:\n",
    "            raise Exception('Data Collection not implemented for country_code: ' + country_code)\n",
    "    except Exception as inst:\n",
    "        print('{}'.format(inst))\n",
    "    \n",
    "    #make sure that date & region_code have the correct data type\n",
    "    covid_source_df_cleansed['date'] = pd.to_datetime(covid_source_df_cleansed['date'])\n",
    "    covid_source_df_cleansed['region_code'] = covid_source_df_cleansed['region_code'].astype('str') \n",
    "\n",
    "    print('\\nCOVID-19 measures in source (after cleanse): ' + str(covid_source_df_cleansed.shape[0]))\n",
    "    return covid_source_df_cleansed\n",
    "    \n",
    "# Create a country specific function to:\n",
    "# a) get the data\n",
    "# b) align the columns to the standard columns (date, region_code, region_name, {metric}_{metric_type}\n",
    "# c) cleans the data \n",
    "def get_NL_covid_data():\n",
    "# a) get the data\n",
    "    covid_source_url = 'https://raw.githubusercontent.com/J535D165/CoronaWatchNL/master/data-json/data-provincial/RIVM_NL_provincial.json'\n",
    "    covid_source_df = pd.DataFrame(requests.get(covid_source_url).json()['data'])\n",
    "# b) map the columns\n",
    "    covid_source_column_dict = {\n",
    "        # map the columns for data, region_code, region_name\n",
    "        'Datum': 'date', 'Provinciecode':'region_code', 'Provincienaam':'region_name',\n",
    "        # map the columns for the avaible metrics\n",
    "        'totaalAantal':'confirmed_addition',\n",
    "        'totaalAantalCumulatief':'confirmed_cumulative',\n",
    "        'ziekenhuisopnameAantal':'hospitalized_addition',\n",
    "        'ziekenhuisopnameAantalCumulatief':'hospitalized_cumulative',\n",
    "        'overledenAantal':'deceased_addition',\n",
    "        'overledenAantalCumulatief': 'deceased_cumulative'}\n",
    "    covid_source_df.rename(columns=covid_source_column_dict, inplace=True)\n",
    "# c) cleans the data\n",
    "\n",
    "# return the data\n",
    "    return covid_source_df\n",
    "\n",
    "def get_FR_covid_data():\n",
    "#   a) get the data\n",
    "    covid_source_url = \"https://raw.githubusercontent.com/opencovid19-fr/data/master/dist/chiffres-cles.csv\"\n",
    "    covid_source_df = pd.read_csv(io.StringIO(requests.get(covid_source_url).content.decode('utf-8')))\n",
    "#   b) map the columns\n",
    "    covid_source_column_dict = {\n",
    "        # map the columns for data, region_code, region_name\n",
    "        'date': 'date','maille_code':'region_code','maille_nom':'region_name',\n",
    "        # map the columns for the avaible metrics\n",
    "        'deces':'deceased_cumulative',\n",
    "        'nouvelles_hospitalisations':'hospitalized_addition',#'hospitalises':'hospitalized_addition',\n",
    "        'nouvelles_reanimations':'hospitalized_icu_addition',#'reanimation':'hospitalized_icu_addition',\n",
    "        'gueris':'recovered_cumulative',\n",
    "        # map other columns used for filtering\n",
    "        'source_nom' : 'source'}\n",
    "    covid_source_df.rename(columns=covid_source_column_dict, inplace=True)\n",
    "#   c) cleans the data\n",
    "    # source includes data on various granularity levels, we only want to keep data on region level\n",
    "    covid_source_df = covid_source_df[covid_source_df['granularite'].eq('region')]\n",
    "\n",
    "    # There can be several entries for the same Region/day if so select the \"Santé publique France\" in the source column.\n",
    "    # NB! \"Santé publique France\" values may be NaN while others sources are not. Only an issue around March 23-25 \n",
    "    covid_source_df = covid_source_df[~covid_source_df.duplicated(['date', 'region_code'], keep=False) | covid_source_df['source'].eq('Santé publique France')]\n",
    "    \n",
    "# return the data\n",
    "    return covid_source_df\n",
    "\n",
    "def get_DK_covid_data(country_file_path):\n",
    "#   a) get the data\n",
    "    covid_source_url = country_file_path + 'Newly_admitted_over_time.csv'\n",
    "    try:\n",
    "        covid_source_df = pd.read_csv(covid_source_url, sep=';')\n",
    "        print('WARN: {} is a local file. Please make sure you download the latest file from the [covid_data_provider]'.format(covid_source_url))\n",
    "    except:\n",
    "        raise Exception('ERROR: Cannot find {}. Please download this file from the [covid_data_provider] to continue.'.format(covid_source_url))\n",
    "        \n",
    "    covid_source_df[\"Dato\"] = pd.to_datetime(covid_source_df[\"Dato\"])\n",
    "    covid_source_df = covid_source_df.drop(['Total'], axis=1)\n",
    "    # NB! Translating region names to English for matching afterwards # org regions: ['Hovedstaden', 'Sjælland', 'Syddanmark','Midtjylland','Nordjylland']\n",
    "    covid_source_df.columns = ['Dato','Capital Region of Denmark', 'Region Zealand', 'Region of Southern Denmark',\n",
    "                                                                           'Central Denmark Region','North Denmark Region']\n",
    "    covid_source_df = pd.melt(covid_source_df, id_vars='Dato', value_vars=['Capital Region of Denmark', 'Region Zealand', 'Region of Southern Denmark',\n",
    "                                                                           'Central Denmark Region','North Denmark Region'], var_name='Region', value_name='Admitted')\n",
    "#   b) map the columns\n",
    "    covid_source_df['region_name'] = ''\n",
    "    covid_source_column_dict = {\n",
    "        # map the columns for data, region_code\n",
    "        'Dato': 'date','Region':'region_code',\n",
    "        # map the columns for the avaible metrics\n",
    "        'Admitted':'hospitalized_addition'\n",
    "    }\n",
    "    covid_source_df.rename(columns=covid_source_column_dict, inplace=True)\n",
    "# c) cleans the data\n",
    "\n",
    "# return the data\n",
    "    return covid_source_df\n",
    "\n",
    "def get_SE_covid_data(country_file_path):\n",
    "#   a) get the data\n",
    "    covid_source_url = country_file_path + 'region.csv'\n",
    "    try:\n",
    "        covid_source_df = pd.read_csv(covid_source_url, sep=',', encoding='utf-8')\n",
    "        print('WARN: {} is a local file. Please make sure you download the latest file from the [covid_data_provider]'.format(covid_source_url))\n",
    "    except:\n",
    "        raise Exception('ERROR: Cannot find {}. Please download this file from the [covid_data_provider] to continue.'.format(covid_source_url))\n",
    "\n",
    "    # rearrange date column\n",
    "    dato = covid_source_df['Statistikdatum']\n",
    "    covid_source_df.drop(labels=['Statistikdatum'], axis=1,inplace = True)\n",
    "    covid_source_df.insert(0, 'Statistikdatum', dato)\n",
    "    covid_source_df[\"Statistikdatum\"] = pd.to_datetime(covid_source_df[\"Statistikdatum\"])\n",
    "    covid_source_df = covid_source_df.drop(['Totalt_antal_fall','Kumulativa_fall','Antal_avlidna','Kumulativa_avlidna','Antal_intensivvardade','Kumulativa_intensivvardade'], axis=1)\n",
    "    # No translating into English according to https://en.wikipedia.org/wiki/ISO_3166-2:SE \n",
    "    # NB! Specifying the new cases column\n",
    "    colnames = list(covid_source_df)\n",
    "    colnames.pop(0) # remove 1st item\n",
    "    covid_source_df = pd.melt(covid_source_df, id_vars='Statistikdatum', value_vars=colnames, var_name='County', value_name='new_cases')\n",
    "#   b) map the columns\n",
    "    covid_source_df['region_name'] = ''\n",
    "    covid_source_column_dict = {\n",
    "        # map the columns for data, region_code\n",
    "        'Statistikdatum': 'date','County':'region_code',\n",
    "        # map the columns for the avaible metrics\n",
    "        'new_cases':'confirmed_addition'\n",
    "    }\n",
    "    covid_source_df.rename(columns=covid_source_column_dict, inplace=True)\n",
    "# c) cleans the data\n",
    "\n",
    "# return the data\n",
    "    return covid_source_df\n",
    "\n",
    "def get_DE_covid_data():\n",
    "#   a) get the data\n",
    "    covid_source_url = \"https://www.arcgis.com/sharing/rest/content/items/f10774f1c63e40168479a1feb6c7ca74/data\"\n",
    "    covid_source_df = pd.read_csv(io.StringIO(requests.get(covid_source_url).content.decode('utf-8')))\n",
    "    col_keep = ['Meldedatum','Bundesland','AnzahlFall','AnzahlTodesfall']\n",
    "    covid_source_df = covid_source_df[col_keep]\n",
    "    covid_source_df[\"Meldedatum\"] = pd.to_datetime(covid_source_df[\"Meldedatum\"])\n",
    "    covid_source_df = covid_source_df.groupby(by=['Meldedatum','Bundesland']).sum().reset_index() # approach to be checked    \n",
    "#   b) map the columns\n",
    "    covid_source_df['region_name'] = ''\n",
    "    covid_source_column_dict = {\n",
    "        # map the columns for data, region_code\n",
    "        'Meldedatum': 'date','Bundesland':'region_code',\n",
    "        # map the columns for the avaible metrics\n",
    "        'AnzahlFall':'confirmed_addition',\n",
    "        'AnzahlTodesfall': 'deceased_addition'}\n",
    "    covid_source_df.rename(columns=covid_source_column_dict, inplace=True)\n",
    "# c) cleans the data\n",
    "\n",
    "# return the data\n",
    "    return covid_source_df\n",
    "\n",
    "def get_US_covid_data():\n",
    "#   a) get the data\n",
    "    covid_source_url = 'https://covidtracking.com/api/v1/states/daily.json'\n",
    "    covid_source_df = pd.DataFrame(requests.get(covid_source_url).json())\n",
    "#   b) map the columns\n",
    "    covid_source_column_dict = {\n",
    "        # map the columns for data, region_code, region_name\n",
    "        'date': 'date', 'fips':'region_code', 'state':'region_name',\n",
    "        # map the columns for the avaible metrics\n",
    "        'positive':'confirmed_cumulative',\n",
    "        'hospitalizedCumulative':'hospitalized_cumulative',\n",
    "        'inIcuCumulative':'hospitalized_icu_cumulative',\n",
    "        'recovered':'recovered_cumulative',\n",
    "        'death': 'deceased_cumulative'}\n",
    "    covid_source_df.rename(columns=covid_source_column_dict, inplace=True)\n",
    "# c) cleans the data\n",
    "    covid_source_df['date'] =  pd.to_datetime(covid_source_df['date'], format='%Y%m%d')\n",
    "\n",
    "# return the data\n",
    "    return covid_source_df\n",
    "\n",
    "def get_IN_covid_data():\n",
    "#   a) get the data\n",
    "    covid_source_url = \"https://api.covid19india.org/csv/latest/state_wise_daily.csv\"\n",
    "    covid_source_df = pd.read_csv(io.StringIO(requests.get(covid_source_url).content.decode('utf-8')))\n",
    "    # dropping TT column (total for the country) and UN (unknown region)\n",
    "    covid_source_df.drop([\"TT\", \"UN\"], axis = 1, inplace = True)\n",
    "    # melt & pivot the columns to arrive at a data set compliant with the metamodel\n",
    "    covid_source_df = pd.melt(covid_source_df, id_vars=['Date', 'Status'], var_name='region_code', value_name='cases')    \n",
    "    covid_source_df = pd.pivot_table(covid_source_df, values='cases', index=['Date', 'region_code'], columns=['Status'], aggfunc=np.sum)\n",
    "    covid_source_df.reset_index(inplace=True)\n",
    "#   b) map the columns\n",
    "    covid_source_df['region_name'] = ''\n",
    "    covid_source_column_dict = {\n",
    "        # map the columns for data, region_code\n",
    "        'Date': 'date','region_code':'region_code',\n",
    "        # map the columns for the avaible metrics\n",
    "        'Confirmed':'confirmed_addition',\n",
    "        'Deceased': 'deceased_addition',\n",
    "        'Recovered': 'recovered_addition',\n",
    "    }\n",
    "    covid_source_df.rename(columns=covid_source_column_dict, inplace=True)\n",
    "# c) cleans the data\n",
    "    return covid_source_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the region_code in of the covid source to iso3166-2 standard\n",
    "#  - this mapping is defined in the country metadata file by the covid_region_code\n",
    "def merge_country_meta_data(covid_source_df_cleansed, country_metadata_df):\n",
    "    country_metadata_df = explode_covid_region_code(country_metadata_df)\n",
    "    covid_source_df_cleansed = covid_source_df_cleansed.merge(country_metadata_df, left_on='region_code', right_on='covid_region_code', how='left')\n",
    "    covid_source_df_cleansed['iso3166-2_code'] = covid_source_df_cleansed['iso3166-2_code'].fillna('?')\n",
    "\n",
    "    if(len(covid_source_df_cleansed[covid_source_df_cleansed['iso3166-2_code'].eq('?')])>0):\n",
    "        print(\"\\nWARN: Not all regions mapped to iso3166-2_code. They will be dropped. These are missing mappings:\\n\")\n",
    "        country_region_mapping = covid_source_df_cleansed[covid_source_df_cleansed['iso3166-2_code'].eq('?')].groupby(['region_code','region_name','iso3166-2_code'])\n",
    "        print(country_region_mapping['date'].max())\n",
    "        covid_source_df_cleansed = covid_source_df_cleansed[~covid_source_df_cleansed['iso3166-2_code'].eq('?')]\n",
    "\n",
    "    print(\"\\nINFO: These regions are mapped to iso3166-2_code:\\n\")\n",
    "    country_region_mapping = covid_source_df_cleansed.groupby(['region_code','region_name','iso3166-2_code'])\n",
    "    print(country_region_mapping['date'].max())\n",
    "        \n",
    "    covid_source_df_cleansed.set_index(['iso3166-2_code'], inplace=True)\n",
    "    covid_source_df_cleansed.sort_values(by=['iso3166-2_code','date'],inplace=True)\n",
    "    \n",
    "    return covid_source_df_cleansed\n",
    "\n",
    "# multiple covid_region_codes can be mapped to one iso3166-2_code\n",
    "# for this add comma separated IDs in the covid_region_code\n",
    "# the explode function splits these IDs and add them as separate rows in the dataframe\n",
    "def explode_covid_region_code(country_metadata_df):\n",
    "    return pd.DataFrame(country_metadata_df['covid_region_code'].str.split(',').tolist(),\n",
    "                index=country_metadata_df['iso3166-2_code']) \\\n",
    "        .stack().to_frame() \\\n",
    "        .reset_index([0, 'iso3166-2_code']) \\\n",
    "        .rename(columns={0: 'covid_region_code'})\\\n",
    "        .merge(country_metadata_df.drop([\"covid_region_code\"], axis = 1), \n",
    "               left_on='iso3166-2_code', right_on='iso3166-2_code', how='inner')\n",
    "\n",
    "    return country_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a Data Frame holding the metrics that are available for the analysis & visualizations\n",
    "def get_available_metrics(covid_source_df):\n",
    "    metrics = ['confirmed','hospitalized','hospitalized_icu','recovered','deceased']\n",
    "    metric_types = ['addition','substraction','absolute','cumulative']\n",
    "    metric_maths = ['','population_weighted']\n",
    "    \n",
    "    available_metrics = pd.DataFrame(columns=['column_name','metric','metric_type','metric_math','label'])\n",
    "    for metric in metrics:\n",
    "        for metric_type in metric_types:\n",
    "            for metric_math in metric_maths:\n",
    "                column_name = metric + '_' + metric_type\n",
    "                column_name += '_' + metric_math if metric_math != '' else ''\n",
    "                if column_name in covid_source_df.columns:\n",
    "                    new_row = {'column_name':column_name,'metric':metric,'metric_type':metric_type,'metric_math':metric_math,'label':metric}\n",
    "                    available_metrics = available_metrics.append(new_row, ignore_index=True)\n",
    "    available_metrics.set_index('column_name', inplace=True)\n",
    "    return available_metrics\n",
    "\n",
    "# add missing metric columns that can be derived from other metrics and remove everything else\n",
    "def cleans_metrics(covid_source_df):\n",
    "    available_metrics = get_available_metrics(covid_source_df)\n",
    "\n",
    "    # only keep the columns we will include in the analysis\n",
    "    covid_source_df = covid_source_df[['date','iso3166-2_name_en','population'] + available_metrics.index.tolist()]\n",
    "\n",
    "    # drop all duplicates\n",
    "    covid_source_df = covid_source_df.drop_duplicates()\n",
    "    print('\\nCOVID-19 measures in source (after dedup): ' + str(covid_source_df.shape[0]))\n",
    "\n",
    "    # fill NaN for [cumulative] measurements\n",
    "    covid_source_cumulative_metrics = available_metrics[(available_metrics.metric_type=='cumulative')].index.tolist()\n",
    "    if(len(covid_source_cumulative_metrics) > 0):\n",
    "        covid_source_df[covid_source_cumulative_metrics] = covid_source_df[covid_source_cumulative_metrics].groupby('iso3166-2_code').fillna(method='ffill')\n",
    "\n",
    "    # derive [addition] from [cumulative] measurements if not already included from the source\n",
    "    #   [addition] metrics already in the source\n",
    "    addition_metrics_available = available_metrics[(available_metrics.metric_type=='addition')].metric.tolist()\n",
    "    #   [cumulative] metrics in the source but no [addition] metric\n",
    "    cumulative_metrics_for_addition = available_metrics[ \\\n",
    "                                        (available_metrics.metric_type=='cumulative') & \\\n",
    "                                        ~(available_metrics.metric.isin(addition_metrics_available))].index.tolist()\n",
    "\n",
    "    if(len(cumulative_metrics_for_addition) > 0):\n",
    "        addition_metrics_to_derive = [cm.replace('cumulative','addition') for cm in cumulative_metrics_for_addition]\n",
    "        covid_source_df[addition_metrics_to_derive] = covid_source_df.groupby(['iso3166-2_code'])[cumulative_metrics_for_addition].apply(lambda x: x.diff())    \n",
    "\n",
    "    # fill 0 for all NaN values\n",
    "    covid_source_df.fillna(value=0, inplace=True)\n",
    "    \n",
    "    # derive _cumulative metrics from _addition if not present\n",
    "    available_metrics = get_available_metrics(covid_source_df)\n",
    "    cumulative_metrics_available = available_metrics[(available_metrics.metric_type=='cumulative')].metric.tolist()\n",
    "    addition_metrics_for_cumulative = available_metrics[ \\\n",
    "                                        (available_metrics.metric_type=='addition') & \\\n",
    "                                        ~(available_metrics.metric.isin(cumulative_metrics_available))].index.tolist()\n",
    "\n",
    "    if(len(addition_metrics_for_cumulative) > 0):\n",
    "        cumulative_metrics_to_derive = [cm.replace('addition','cumulative') for cm in addition_metrics_for_cumulative]\n",
    "        covid_source_df[cumulative_metrics_to_derive] = covid_source_df.groupby(['iso3166-2_code'])[addition_metrics_for_cumulative].cumsum()\n",
    "    \n",
    "    ## calculate population weighted for all metrics\n",
    "    available_metrics = get_available_metrics(covid_source_df)\n",
    "    for metric in available_metrics.index.tolist():\n",
    "        metric_pw = metric + '_population_weighted'\n",
    "        covid_source_df[metric_pw]=covid_source_df[metric]/covid_source_df['population']\n",
    "    \n",
    "    available_metrics = get_available_metrics(covid_source_df)\n",
    "    return covid_source_df, available_metrics\n",
    "\n",
    "def get_metrics_to_analyse(available_metrics, analysis):\n",
    "    metrics_to_analyse_available = []\n",
    "    metrics_to_analyse_not_available = []\n",
    "\n",
    "    for outcome in analysis['model']['outcome']:\n",
    "        metric_found = False\n",
    "        for available_metric in available_metrics.index:\n",
    "            if re.fullmatch(outcome, available_metric):\n",
    "                metrics_to_analyse_available.append(available_metric)\n",
    "                metric_found = True\n",
    "        if not(metric_found):\n",
    "            metrics_to_analyse_not_available.append(outcome)\n",
    "\n",
    "    if(len(metrics_to_analyse_not_available) != 0):\n",
    "        print('WARN: These metrics are not available for analysis: {}'.format(metrics_to_analyse_not_available))\n",
    "\n",
    "    if(len(metrics_to_analyse_available) == 0):\n",
    "        raise Exception('ERROR: There are no metrics available for analysis')\n",
    "    else:\n",
    "        print('INFO: These metrics will be analysed: {}'.format(metrics_to_analyse_available))\n",
    "\n",
    "    return available_metrics[available_metrics.index.isin(metrics_to_analyse_available)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Get the country COVID-19 source and clean the data to a standard format ##\n",
    "covid_source_df_cleansed          = get_country_covid_data(ANALYSIS)\n",
    "covid_source_df_cleansed          = merge_country_meta_data(covid_source_df_cleansed,COUNTRY_METADATA_DF)\n",
    "COVID_SOURCE_DF,available_metrics = cleans_metrics(covid_source_df_cleansed)\n",
    "\n",
    "## Determine what metrics we want to analyse are in the country input ##\n",
    "ANALYSIS['available_metrics']     = get_metrics_to_analyse(available_metrics,ANALYSIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Get IBM PAIRS Geospatial & Temporal data\n",
    "\n",
    "The **ANALYSIS** definition with the **COUNTRY_METADATA** file is used to control:\n",
    "- What geospatial filter is applied (i.e. NL Provinces with geometry from a shapefile)\n",
    "- What temporal filter is applied (i.e. the Time Window with consideration of Rolling Windows & Time Shifts)\n",
    "- What data set filter is applied (i.e. the IBM PAIRS data layer(s))\n",
    "- What temporal aggregation is applied (i.e. how IBM PAIRS aggregates the data)\n",
    "\n",
    "The code below will:\n",
    "- Construct the IBM PAIRS Query\n",
    "- Submit the query & process the results\n",
    "- Cache the results\n",
    "\n",
    "Caching the result is important since the (initial) data retrieval can be a process that can take hour(s). The duration is depended on the size of the geography, the length of the time period and the data set(s) requested. Therefore the results are cached to a local file. The cache file is checked when running a new analysis and only the missing data (if any) is retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_agg_column(datum, aggregation):\n",
    "    return '{}_data_agg_{}'.format(datum,aggregation)\n",
    "\n",
    "def data_points_column(datum):\n",
    "    return '{}_data_points'.format(datum)\n",
    "\n",
    "def data_avg_column(datum, aggregation):\n",
    "    return '{}_data_avg_{}'.format(datum,aggregation)\n",
    "\n",
    "def init_pairs_cache(pairs_query):\n",
    "    filepath = pairs_query['cache_file']\n",
    "    if os.path.isfile(filepath):\n",
    "        # retrieve the cached results\n",
    "        print('Found PAIRS_QUERY_CACHE_FILE: ' + filepath)\n",
    "        cache_df=pd.read_csv(filepath)\n",
    "        cache_df[\"date\"] = pd.to_datetime(cache_df[\"date\"])\n",
    "    else:\n",
    "        print('Did not find PAIRS_QUERY_CACHE_FILE: ' + filepath)\n",
    "        # create a dataframe with date, region (use the same column name as in source COVID data)\n",
    "        column_names = [\"date\", \"iso3166-2_code\"]\n",
    "        cache_df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "    cache_df[\"date\"] = pd.to_datetime(cache_df[\"date\"])\n",
    "    \n",
    "    # add the names of the columns we will use\n",
    "    for alias, layer in pairs_query['layers'].items():\n",
    "        pairs_query['layers'][alias]['data_points_column'] = data_points_column(alias)\n",
    "        pairs_query['layers'][alias]['data_agg_column'] = data_agg_column(alias,layer['aggregation'])\n",
    "        pairs_query['layers'][alias]['data_avg_column'] = data_avg_column(alias,layer['aggregation'])\n",
    "    return cache_df\n",
    "\n",
    "# query that creates one layer for each day in the list of provided days for the layer ID & poly provided \n",
    "def create_pairs_query_by_poly(pairs_query,poly):\n",
    "    return create_pairs_query(pairs_query, 'polygon', poly)\n",
    "    \n",
    "def create_pairs_query(pairs_query,geo_type,geo_data):\n",
    "    # The following helps when converting datetime objects to strings in ISO 8601-compliant format.\n",
    "    iso8601 = '%Y-%m-%dT%H:%M:%SZ'\n",
    "    \n",
    "    queryJson = {\n",
    "        \"layers\": [],\n",
    "        \"name\": pairs_query['alias']\n",
    "    }\n",
    "\n",
    "    if(geo_type==\"polygon\"):\n",
    "        queryJson['spatial'] = {\"type\" :\"poly\", \"polygon\": {\"wkt\": geo_data}}\n",
    "    else:\n",
    "        raise Exception(\"Geo Type not implemented\")\n",
    "    \n",
    "    outer_dates = []\n",
    "    for alias, layer in pairs_query['layers'].items():\n",
    "        outer_dates.append(layer['days_to_collect'].min())\n",
    "        outer_dates.append(layer['days_to_collect'].max())\n",
    "        \n",
    "        for startdate in layer['days_to_collect']:     \n",
    "            enddate=startdate+timedelta(days=1)\n",
    "            startISO=startdate.strftime(iso8601)\n",
    "            endISO=enddate.strftime(iso8601)\n",
    "\n",
    "            queryJson['layers'].append({\n",
    "                \"alias\": alias + \"_\" + startdate.strftime('%Y%m%d'),\n",
    "                \"id\": layer['layer_id'],\n",
    "                \"output\": True,\n",
    "                \"aggregation\" : layer['aggregation'],\n",
    "                \"type\": \"raster\",\n",
    "                \"temporal\": {\"intervals\": [{\"start\": startISO,\"end\": endISO}]}\n",
    "            })\n",
    "\n",
    "    firstdate = min(outer_dates)\n",
    "    lastdate = max(outer_dates) + timedelta(days=1)\n",
    "    queryJson['temporal'] = {\"intervals\": [{\"start\": firstdate.strftime(iso8601), \"end\": lastdate.strftime(iso8601)}]}\n",
    "    \n",
    "    return queryJson\n",
    "\n",
    "def update_pairs_cache(pairs_cache, pairs_server, pairs_credentials, pairs_query, date_range, country_metadata_df):\n",
    "    start_time_overall = timer()\n",
    "    total_regions = country_metadata_df.shape[0]\n",
    "    print(\"Start IBM PAIRS Queries for {} regions.\\n\".format(total_regions))\n",
    "    region_id = 0\n",
    "    for index, region in country_metadata_df.iterrows():\n",
    "        region_code = region['iso3166-2_code']\n",
    "        region_id += 1\n",
    "\n",
    "        days_to_collect = 0\n",
    "        # determine the missing dates that are not yet in the cached results per layer\n",
    "        for alias, layer in pairs_query['layers'].items():\n",
    "            # we assume we need to get all days\n",
    "            diff_data_range = date_range\n",
    "            # but for columns already in the cache we only pull missing values\n",
    "            if layer['data_agg_column'] in pairs_cache.columns:\n",
    "                cached_date_range = pairs_cache[\\\n",
    "                    (pairs_cache['iso3166-2_code']==region_code) &\\\n",
    "                    (pairs_cache[layer['data_agg_column']].notna())]['date']\n",
    "                diff_data_range = date_range.difference(cached_date_range)\n",
    "                \n",
    "            layer['days_to_collect'] = diff_data_range\n",
    "            days_to_collect += len(diff_data_range)\n",
    "\n",
    "        if(days_to_collect > 0):\n",
    "            print(\"Start IBM PAIRS Queries for: {} ({} of {}).\".format(region_code,region_id,total_regions))\n",
    "            for alias, layer in pairs_query['layers'].items():\n",
    "                print(\" - \" + alias + \": \" + str(len(layer['days_to_collect'])) + \" day(s) to collect.\")               \n",
    "            start_time_region = timer()\n",
    "        else:\n",
    "            print(\"Skip IBM PAIRS Queries for: {} ({} of {}) (No days to collect).\".format(region_code,region_id,total_regions))\n",
    "            continue\n",
    "\n",
    "        query_poly=region['geometry_wkt']\n",
    "        queryJSON=create_pairs_query_by_poly(pairs_query, query_poly)\n",
    "        logging.getLogger('ibmpairs.paw').setLevel(logging.ERROR)\n",
    "        query = paw.PAIRSQuery(queryJSON, pairs_server, pairs_credentials)\n",
    "        start_time_query = timer()\n",
    "        print(\" Submit query. Total elapse time: \" + str(round((start_time_query - start_time_overall),1)))\n",
    "        query.submit()\n",
    "        query.poll_till_finished()\n",
    "        end_time_query = timer()\n",
    "        queryStatus = query.queryStatus.json()\n",
    "        if(queryStatus['status'] == 'Succeeded'):\n",
    "            print(\" Download query result. Query elapse time: \" + str(round((end_time_query - start_time_query),1)))        \n",
    "            query.download()\n",
    "            query.create_layers()\n",
    "        else:\n",
    "            print(\" No download. PAIRS Query Status: \" + queryStatus['status'] + \"\\n\")\n",
    "            continue\n",
    "\n",
    "        # the query returns one layer per day\n",
    "        query_metadata = pd.DataFrame(query.metadata).transpose()\n",
    "        for dla in query_metadata['datalayerAlias']:\n",
    "            query_meta = query_metadata[(query_metadata['datalayerAlias'] == dla)]           \n",
    "            data_idx = query_meta.index[0]\n",
    "            temporalAggregation = query_metadata.at[data_idx,'temporalAggregation']\n",
    "            \n",
    "            layer_alias = dla.split('_')[0]\n",
    "            layer_date = datetime.strptime(dla.split('_')[1],'%Y%m%d')\n",
    "            print(\"   - Layer returned for [\" + layer_alias + \"] for date: \" + layer_date.strftime(\"%Y-%m-%d\"))\n",
    "            layer_data = query.data[data_idx]\n",
    "            # non nan values correspond to measurement points in the area of interest, nan values are outside.\n",
    "            # nanmean computes the average of weather data per surface within the area of interest\n",
    "            # add 0.001 to avoid divide by zero.\n",
    "            data_points=0\n",
    "            data_sum=0\n",
    "            data_points+=np.count_nonzero(~np.isnan(layer_data))\n",
    "            data_sum=np.nansum(layer_data)\n",
    "            data_avg=data_sum/(data_points+0.001)\n",
    "\n",
    "            #column names in the cache file use the layer_alias as prefix\n",
    "            data_points_col = data_points_column(layer_alias)\n",
    "            data_agg_col = data_agg_column(layer_alias,temporalAggregation)\n",
    "            data_avg_col = data_avg_column(layer_alias,temporalAggregation)\n",
    "\n",
    "            if(data_sum > 0): # is data_sum is 0 it means there are no results and we skip adding the row\n",
    "                if(pairs_cache[(pairs_cache['iso3166-2_code']==region_code) & (pairs_cache['date']==layer_date)].empty):\n",
    "                    #append row to the dataframe\n",
    "                    layer_row = pd.Series(data={'date':layer_date, 'iso3166-2_code':region_code, data_agg_col: data_sum, data_points_col: data_points, data_avg_col:data_avg})\n",
    "                    pairs_cache = pairs_cache.append(layer_row, ignore_index=True)\n",
    "                else:\n",
    "                    #update row on dataframe\n",
    "                    layer_row = pairs_cache[(pairs_cache['iso3166-2_code']==region_code) & (pairs_cache['date']==layer_date)]\n",
    "                    pairs_cache.at[(pairs_cache['iso3166-2_code']==region_code) & (pairs_cache['date']==layer_date),\n",
    "                                        data_agg_col] = data_sum\n",
    "                    pairs_cache.at[(pairs_cache['iso3166-2_code']==region_code) & (pairs_cache['date']==layer_date),\n",
    "                                        data_points_col] = data_points\n",
    "                    pairs_cache.at[(pairs_cache['iso3166-2_code']==region_code) & (pairs_cache['date']==layer_date),\n",
    "                                        data_avg_col] = data_avg\n",
    "        # do a final save of the CSV with safety measure to make sure no duplicate entries are stored\n",
    "        print(\"Finished IBM PAIRS Queries for: \" + region_code + \". Save results to cache file.\\n\")\n",
    "        pairs_cache.drop_duplicates(inplace=True)\n",
    "        pairs_cache.to_csv(pairs_query['cache_file'], index=False)\n",
    "    print(\"Finished all IBM PAIRS Queries.\\n\")\n",
    "    return pairs_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the cached results previousy obtained from PAIRS\n",
    "PAIRS_CACHE_DF = init_pairs_cache(PAIRS_QUERY)\n",
    "\n",
    "# get additional data from PAIRS or skip to only use the local cached data\n",
    "if(PAIRS_CACHE_REFRESH):\n",
    "    print(\"\\nCollect data from \" + PAIRS_QUERY['start_date'].strftime(\"%Y-%m-%d\") + \" up to and including \" + PAIRS_QUERY['end_date'].strftime(\"%Y-%m-%d\"))\n",
    "    # create the date range, in daily increments, for the data we want from PAIRS\n",
    "    requested_date_range = pd.date_range(start=PAIRS_QUERY['start_date'],end=PAIRS_QUERY['end_date'],freq='D')\n",
    "\n",
    "    # query pairs & update the cache\n",
    "    PAIRS_CACHE_DF = update_pairs_cache(\n",
    "        PAIRS_CACHE_DF, PAIRS_SERVER, PAIRS_CREDENTIALS, PAIRS_QUERY, requested_date_range, COUNTRY_METADATA_DF)\n",
    "else:\n",
    "    print('\\nSkip datacollect from IBM PAIRS. Only use cache.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Add Rolling Windows & Time Shifts\n",
    "The **ANALYSIS** definition controls what *Rolling Windows* and *Time Shifts* are used. These parameters allow the algorithm to be tuned to e.g. incorporate the influence of different incubation times.\n",
    "\n",
    "A new column is added to the PAIRS_CACHE_DF for each *Rolling Window* and each *Time Shift* specified in the ANALYSIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to construct the column names for the Rolling Window and Time Shift columns\n",
    "def time_shift_label(shift):\n",
    "    return \"TS\" + str(shift) + \"D\"\n",
    "\n",
    "def time_shift_labels(time_shifts):\n",
    "    shift_labels = []\n",
    "    for shift in time_shifts:\n",
    "        shift_labels.append(time_shift_label(shift))\n",
    "    return shift_labels\n",
    "\n",
    "def rolling_window_column(metric,window,window_type):\n",
    "    return metric + '_rolling_' + window_type + '_' + str(window) + 'D'\n",
    "\n",
    "def time_shift_column(metric,window,window_type,shift):\n",
    "    return rolling_window_column(metric,window,window_type) + '_' + time_shift_label(shift)\n",
    "\n",
    "# Add a rolling window column for each of the specified window_sizes.\n",
    "# Shift the rolling windows for each of the time_shifts to introduce time-lagged correlation\n",
    "def add_windows(pairs_cache, analysis):\n",
    "    pairs_cache = pairs_cache.sort_values(by='date')\n",
    "    new_pairs_cache = pd.DataFrame()\n",
    "    output_columns = ['date','iso3166-2_code'] +\\\n",
    "                        list(map(lambda x: data_avg_column(x,analysis['pairs_query']['layers'][x]['aggregation']),\\\n",
    "                                           analysis['model']['predictor'])) \n",
    "    for region in pairs_cache['iso3166-2_code'].unique():\n",
    "        pairs_region = pairs_cache[pairs_cache['iso3166-2_code']==region][output_columns].copy()\n",
    "\n",
    "        for predictor in analysis['model']['predictor']:\n",
    "            metric = data_avg_column(predictor,analysis['pairs_query']['layers'][predictor]['aggregation'])\n",
    "            rolling_windows = analysis['model']['rolling_windows']\n",
    "            rolling_window_type = analysis['model']['rolling_window_type']\n",
    "            time_shifts = analysis['model']['time_shifts']\n",
    "\n",
    "            # calculate the rolling window\n",
    "            for window in rolling_windows:\n",
    "                rolling_window_name = rolling_window_column(metric,window,rolling_window_type)\n",
    "                if(rolling_window_type=='mean'):\n",
    "                    pairs_region[rolling_window_name] = pairs_region.rolling(window)[metric].mean()\n",
    "                elif(rolling_window_type=='sum'):\n",
    "                    pairs_region[rolling_window_name] = pairs_region.rolling(window)[metric].sum()\n",
    "                else:\n",
    "                    raise Exception('Rolling Window Type not defined: ' + rolling_window_type)\n",
    "\n",
    "                # shift the rolling sum for each time_shift to do an easy time-lagged correlation\n",
    "                for shift in time_shifts:\n",
    "                    time_shift_name = time_shift_column(metric,window,rolling_window_type,shift)\n",
    "                    pairs_region[time_shift_name] = pairs_region[rolling_window_name].shift(shift)\n",
    "        new_pairs_cache = new_pairs_cache.append(pairs_region)\n",
    "    \n",
    "    #want to store it separate from the cache file so that rolling sum columns are recreated everytime\n",
    "    new_pairs_cache.to_csv(analysis['cache_file'], index=False)\n",
    "    \n",
    "    return new_pairs_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the rolling window(s) and set the time_shifts for time-lagged correlation\n",
    "print('\\nAdd Rolling Windows & Time Shifts.')\n",
    "PAIRS_CACHE_DF = add_windows(PAIRS_CACHE_DF, ANALYSIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Merge COVID-19 & IBM PAIRS datasets\n",
    "\n",
    "At this stage we have available:\n",
    "\n",
    "- **COVID_PAIRS_DF**: Country Metadata & COVID-19 dataset in a standardized format\n",
    "- **PAIRS_CACHE_DF**: Geospatial dataset from PAIRS for the country & time window required\n",
    "\n",
    "These two data sets are merged on the *date* and *iso3166-2 code* so that they can be analyzed & visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the IBM PAIRS query results to the COVID-19 source\n",
    "print('\\nCOVID-19 measures in source (cleansed): ' + str(COVID_SOURCE_DF.shape[0]))\n",
    "COVID_PAIRS_DF = COVID_SOURCE_DF.merge(PAIRS_CACHE_DF, on=['iso3166-2_code','date'])\n",
    "print('COVID-19 measures in source with ' + PAIRS_QUERY['alias'] + ': ' + str(COVID_PAIRS_DF.shape[0]))\n",
    "\n",
    "# take the time slice to analyse & visualize\n",
    "COVID_PAIRS_DF_TIME_SLICED = \\\n",
    "    COVID_PAIRS_DF[ \\\n",
    "     (COVID_PAIRS_DF['date'] >= pd.to_datetime(ANALYSIS['time_window']['window_start'])) & \\\n",
    "     (COVID_PAIRS_DF['date'] <= pd.to_datetime(ANALYSIS['time_window']['window_end']))]\n",
    "\n",
    "COVID_PAIRS_DF.to_csv(COUNTRY_FILE_PATH + \"COVID_PAIRS_DF.csv\",index=True)\n",
    "COVID_PAIRS_DF_TIME_SLICED.to_csv(COUNTRY_FILE_PATH + \"COVID_PAIRS_DF_TIME_SLICED.csv\",index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Determine Spearman or GAM\n",
    "The selected **MODEL** in the **ANALYSIS** definition controls the type of algorithm that is run. This Notebook supports two different models:\n",
    "\n",
    "### 1. Spearman\n",
    "The coefficient is determined for the *predictor* and each of the *outcomes* defined in the model.\n",
    "\n",
    "The correlation between these two variables is then determined for each *rolling_window* and each *time_shift*. The *alpha* variable determines whether the correlation is deemed statistically significant or not.\n",
    "\n",
    "### 2. Generalized Additive Model (GAM)\n",
    "The log-linear association is determined for each of the *independent_variables* and each of the *outcomes*, whereby the *control_variables* are used as [confounding variables](https://en.wikipedia.org/wiki/Confounding) which are variables, other than the independent variables in focus, that may affect the outcome and thus, may lead to erroneous conclusions about the relationship between the independent and outcome variables.\n",
    "\n",
    "The association between the *independent_variables* and each of the *outcomes* is determined for each *rolling_window* and each *time_shift*. The *alpha* variable determines whether the association is deemed statistically significant or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to construct the column names for the Rolling Window and Time Shift columns\n",
    "def analysis_result_key(outcome, predictor, window):\n",
    "    return outcome + \"_\"+ predictor + \"_rolling_\" + str(window) + \"D\"\n",
    "\n",
    "def get_spearman_correlations(analysis, covid_pairs_df):\n",
    "    analysis_result_datasets = {}\n",
    "    \n",
    "    # Fix the predictor to the first specified in the model.\n",
    "    # TODO: Support use of multiple predictors\n",
    "    predictor = analysis['model']['predictor'][0]\n",
    "    predictor_column_name = data_avg_column(predictor,analysis['pairs_query']['layers'][predictor]['aggregation'])\n",
    "    \n",
    "    # Determine the correlation for each outcome & each rolling window\n",
    "    for metric_column_name, metric_row in analysis['available_metrics'].iterrows():\n",
    "        print('\\nPerform Spearman correlation for: ' + metric_column_name)\n",
    "        rank_variable_one = metric_column_name\n",
    "\n",
    "        for window in analysis['model']['rolling_windows']:\n",
    "            rank_variable_two = rolling_window_column(predictor_column_name,\\\n",
    "                                                      window,analysis['model']['rolling_window_type'])\n",
    "            # determine the correlation\n",
    "            analysis_df = determine_spearman_correlation(\n",
    "                covid_pairs_df, rank_variable_one, rank_variable_two, \n",
    "                analysis['model']['time_shifts'], analysis['model']['alpha'])\n",
    "            \n",
    "            # add the analysis_results to the analysis_result_datasets dictionary\n",
    "            result_key = analysis_result_key(metric_column_name,predictor,window)\n",
    "            analysis_result_datasets[result_key] = \\\n",
    "                dict(model = 'spearman', \n",
    "                     label = metric_row.label,\n",
    "                     rank_variable_one = rank_variable_one,\n",
    "                     rank_variable_two = rank_variable_two,\n",
    "                     predictor = predictor, \n",
    "                     significant_field = 'rho', \n",
    "                     significant_field_display = 'rho',\n",
    "                     data = analysis_df)\n",
    "\n",
    "            \n",
    "            # write the results to a local file\n",
    "            analysis_df.to_csv(\\\n",
    "                analysis['country_file_path'] + \"spearman_\" + result_key + \".csv\",index=True)\n",
    "\n",
    "    return analysis_result_datasets\n",
    "\n",
    "# Function to determine the spearman correlation coefficient between two rank variables for each time_shift\n",
    "def determine_spearman_correlation(covid_pairs_df, rank_variable_one, rank_variable_two, time_shifts, alpha):\n",
    "    analysis_df = pd.DataFrame(covid_pairs_df['iso3166-2_code'].unique())\n",
    "    analysis_df.columns = ['iso3166-2_code']\n",
    "    analysis_df.set_index(['iso3166-2_code'], inplace=True)\n",
    "    \n",
    "    rankVariableTwos = []\n",
    "    \n",
    "    for shift in time_shifts:\n",
    "        shift_label = time_shift_label(shift)\n",
    "        rankVariableTwos.append({\"variable\" : rank_variable_two + \"_\" + shift_label, \"time_shift\" : shift, \"label\" : shift_label})\n",
    "\n",
    "    for rankVarTwo in rankVariableTwos:        \n",
    "        for key, data in covid_pairs_df.groupby('iso3166-2_code'):\n",
    "            df_clean = data[[rank_variable_one,rankVarTwo['variable']]].dropna().copy()\n",
    "            \n",
    "            if df_clean.empty:\n",
    "                print('No input for Spearman Correlation: ', key, rank_variable_one, rankVarTwo['variable'])\n",
    "            else:\n",
    "                rho, pval = spearmanr(df_clean[rank_variable_one], df_clean[rankVarTwo['variable']])\n",
    "                analysis_df.loc[key, rankVarTwo['label'] + '_rho'] = rho\n",
    "                analysis_df.loc[key, rankVarTwo['label'] + '_pval'] = pval\n",
    "                analysis_df.loc[key, rankVarTwo['label'] + '_significant'] = ('Y' if pval < alpha else 'N') \n",
    "\n",
    "    return analysis_df\n",
    "\n",
    "def get_gam_associations(analysis, covid_pairs_df):\n",
    "    analysis_result_datasets = {}\n",
    "\n",
    "    # Determine the association for each outcome & each rolling window\n",
    "    for metric_column_name, metric_row in analysis['available_metrics'].iterrows():\n",
    "        print('\\nPerform log-linear GM assocations for: ' + metric_column_name)\n",
    "\n",
    "        for window in analysis['model']['rolling_windows']:\n",
    "            # determine the association\n",
    "            analysis_gam_df = gamModel.determineGam(\n",
    "                gamModel.convertPDtoR(covid_pairs_df,metric_column_name, analysisHelper.PredictorToColumn(analysis)),\n",
    "                analysis['model']['independent_variables'],\n",
    "                analysis['model']['control_variables'],\n",
    "                analysis['model']['time_shifts'],\n",
    "                analysis['model']['rolling_window_type'],\n",
    "                window,\n",
    "                analysis['model']['alpha'])\n",
    "            \n",
    "            # gam model has multiple predictors, so we must unwrap them to obtain results per predictor\n",
    "            for predictor in analysis['model']['independent_variables']:\n",
    "                analysis_df = convert_gam_association(\n",
    "                                    covid_pairs_df,\n",
    "                                    analysis_gam_df.loc[(analysis_gam_df.predictor == predictor)],\n",
    "                                    analysis['model']['time_shifts'])\n",
    "\n",
    "                # add the analysis_results to the analysis_result_datasets dictionary\n",
    "                result_key = analysis_result_key(metric_column_name,predictor,window)\n",
    "                analysis_result_datasets[result_key] = \\\n",
    "                    dict(model = 'gam', \n",
    "                         label = metric_row.label, \n",
    "                         predictor = predictor, \n",
    "                         significant_field = 'coeff', \n",
    "                         significant_field_display = 'perc_change',\n",
    "                         data = analysis_df)\n",
    "\n",
    "                # write the results to a local file\n",
    "                analysis_df.to_csv(\\\n",
    "                    analysis['country_file_path'] + \"gam_\" + result_key + \".csv\",index=True)\n",
    "\n",
    "    return analysis_result_datasets\n",
    "\n",
    "# Function to convert the outcome of the assocation to the format that fits DF format used for visualization\n",
    "# Conversion is done to enable re-use of visualization logic\n",
    "def convert_gam_association(covid_pairs_df, analysis_gam_df, time_shifts):\n",
    "    # Create a table with all ISO3166-2_codes\n",
    "    analysis_df = pd.DataFrame(covid_pairs_df['iso3166-2_code'].unique())\n",
    "    analysis_df.columns = ['iso3166-2_code']\n",
    "    analysis_df.set_index(['iso3166-2_code'], inplace=True)\n",
    "    \n",
    "    for shift in time_shifts:\n",
    "        shift_label = time_shift_label(shift)\n",
    "        significant_results = analysis_gam_df.loc[(analysis_gam_df.time_shift == str(shift))]\n",
    "\n",
    "        for key, data in analysis_df.iterrows():\n",
    "            significant_result = significant_results.loc[(analysis_gam_df['iso3166-2_code'] == key)]\n",
    "            if(significant_result.empty):\n",
    "                analysis_df.loc[key, shift_label + '_significant'] = 'N'\n",
    "                analysis_df.loc[key, shift_label + '_pval'] = np.nan\n",
    "                analysis_df.loc[key, shift_label + '_coeff'] = np.nan\n",
    "                analysis_df.loc[key, shift_label + '_perc_change'] = np.nan\n",
    "            else:\n",
    "                analysis_df.loc[key, shift_label + '_significant'] = 'Y'\n",
    "                analysis_df.loc[key, shift_label + '_pval'] = float(significant_result['p_val'].values[0])\n",
    "                analysis_df.loc[key, shift_label + '_coeff'] = float(significant_result['coeff'].values[0])\n",
    "                analysis_df.loc[key, shift_label + '_perc_change'] = float(significant_result['perc_change'].values[0])\n",
    "            \n",
    "    return analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all the _addition metrics available in the data set we will establish a correlation\n",
    "if(ANALYSIS['model']['model'] == 'gam'):\n",
    "    ANALYSIS_RESULTS_DATASETS = get_gam_associations(ANALYSIS,COVID_PAIRS_DF_TIME_SLICED)\n",
    "elif(ANALYSIS['model']['model'] == 'spearman'):\n",
    "    ANALYSIS_RESULTS_DATASETS = get_spearman_correlations(ANALYSIS,COVID_PAIRS_DF_TIME_SLICED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Visualizations\n",
    "To present the results of the analysis three types of plots are created:\n",
    "\n",
    "1. **Overview Regions**: Line Charts to provide insight in the time series data for the predictor & outcome values\n",
    "2. **Scatter Plot Matrix (spearman only)**: Scatter Plots to provide insight in the potential correlation between predictor & outcome\n",
    "3. **Choropleth Maps**: Maps to show whether a Region shows a significant correlation between the predictor & outcome\n",
    "\n",
    "A plot is created for each *predictor*, *outcome*, *time shift* and *rolling window*.\n",
    "\n",
    "NB: Please note that the charts are interactive. By default the Overview Regions & SPLOM hide the charts for all regions. Selecting the Region Code in the legend will show the corresponding chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template to control standard layout of plots\n",
    "pio.templates[\"gaa_template\"] = go.layout.Template(\n",
    "    layout_margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0},\n",
    "    layout_font_size=8,\n",
    "    layout_title=dict(x=0,font_size=16),\n",
    "    layout_legend=dict(title_font_size=10, font_size=8),\n",
    ")\n",
    "\n",
    "# helper function to map the region codes to a color that is consistent on various plots\n",
    "# colors used: https://www.carbondesignsystem.com/data-visualization/color-palettes#categorical-palettes\n",
    "def categorical_color_map(names):\n",
    "    color_palette = cycle([\n",
    "        '#6929c4', # Purple 70\n",
    "        '#1192e8', # Cyan 50\n",
    "        '#005d5d', # Teal 70\n",
    "        '#9f1853', # Magenta 70 \n",
    "        '#fa4d56', # Red 50\n",
    "        '#520408', # Red 90\n",
    "        '#198038', # Green 60\n",
    "        '#002d9c', # Blue 80\n",
    "        '#ee5396', # Magenta 50\n",
    "        '#b28600', # Yellow 50\n",
    "        '#009d9a', # Teal 50\n",
    "        '#012749', # Cyan 90\n",
    "        '#8a3800', # Orange 70\n",
    "        '#a56eff', # Purple 50\n",
    "    ])\n",
    "    return dict(zip(names, color_palette))\n",
    "    \n",
    "# 3-colorscale (green, white, red) using IBM Design Language colors: https://www.ibm.com/design/language/color/\n",
    "def three_color_scale():\n",
    "    return [(0.00, 'rgb(36, 161, 72)'), (0.33, 'rgb(36, 161, 72)'),       # Green 50\n",
    "              (0.33, 'rgb(255, 255, 255)'), (0.66, 'rgb(255, 255, 255)'), # White\n",
    "              (0.66, 'rgb(218, 30, 40)'), (1.00, 'rgb(218, 30, 40)')]     # Red 60\n",
    "\n",
    "# helper function to create a named trace with consistent appearance & behaviour \n",
    "def create_trace_groups(trace_names, region_color_map):\n",
    "    trace_groups = {\n",
    "        name: {'name': name, 'legendgroup': name, 'line': {'color': region_color_map[name]}}\n",
    "        for name in trace_names\n",
    "    }\n",
    "    return trace_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a series of plots that show the time series for each region:\n",
    "#   - predictors per rolling window\n",
    "#   - available metrics contained in the analysis\n",
    "def create_region_overview_plots(covid_pairs_df, analysis,\n",
    "                                 cases_by_population, region_color_map, row_height = 100, show_regions = True):\n",
    "    # define a title for the overall plot\n",
    "    title = \"<b>Overview Regions {}</b>: {} - {}\".\\\n",
    "        format(analysis['country_code'],analysis['time_window']['window_start'].strftime('%d-%m-%Y'),\\\n",
    "               analysis['time_window']['window_end'].strftime('%d-%m-%Y'))\n",
    "    \n",
    "    # create a title for each subplot\n",
    "    subplot_titles = []\n",
    "    rows = 0\n",
    "    predictors = analysis['model']['predictor']\n",
    "    rolling_windows = analysis['model']['rolling_windows']\n",
    "    \n",
    "    for predictor in predictors:\n",
    "        for window in rolling_windows:\n",
    "            subplot_titles.append(predictor + \" (Rolling Window: \" + str(window) + \"D)\")\n",
    "            rows += 1\n",
    "\n",
    "    for metric_column_name, metric_row in analysis['available_metrics'].iterrows():\n",
    "        if(metric_row.metric_math == 'population_weighted'):\n",
    "            subplot_titles.append(\"{} ({} by {:,} people)\".\\\n",
    "                                  format(metric_row.metric, metric_row.metric_type, cases_by_population))\n",
    "        else:\n",
    "                subplot_titles.append(\"{} ({})\".\\\n",
    "                                  format(metric_row.metric, metric_row.metric_type))\n",
    "        rows += 1\n",
    "    \n",
    "    # create the subplot figure\n",
    "    fig = make_subplots(rows, 1, subplot_titles=subplot_titles,\n",
    "                        vertical_spacing=0.05, horizontal_spacing=0.05,\n",
    "                        shared_xaxes=True, shared_yaxes=False)\n",
    "    \n",
    "    # create a style object so that each region is one tracegroup so we can link color & interaction\n",
    "    trace_groups = create_trace_groups(covid_pairs_df['iso3166-2_code'].unique(), region_color_map)\n",
    "    \n",
    "    # for each region create the traces\n",
    "    for region_code in covid_pairs_df['iso3166-2_code'].unique():\n",
    "        row_id = 1\n",
    "        df_for_region = covid_pairs_df[covid_pairs_df['iso3166-2_code'] == region_code]\n",
    "\n",
    "        for predictor in predictors:\n",
    "            predictor_column_name = data_avg_column(predictor,analysis['pairs_query']['layers'][predictor]['aggregation'])\n",
    "            for window in rolling_windows:\n",
    "                fig.add_trace(go.Scatter(mode='lines',\n",
    "                    x=df_for_region['date'],\n",
    "                    y=df_for_region[rolling_window_column(predictor_column_name,window,analysis['model']['rolling_window_type'])],\n",
    "                    **trace_groups[region_code],showlegend=(row_id==1),visible=show_regions),row_id,1)\n",
    "                row_id += 1\n",
    "        \n",
    "        for metric_column_name, metric_row in analysis['available_metrics'].iterrows():\n",
    "            if(metric_row.metric_math == 'population_weighted'):\n",
    "                y_value = round(df_for_region[metric_column_name]*cases_by_population,0)\n",
    "            else:\n",
    "                y_value = round(df_for_region[metric_column_name],0)\n",
    "                \n",
    "            fig.add_trace(go.Scatter(mode='lines+markers',\n",
    "                x=df_for_region['date'],\n",
    "                y=y_value,\n",
    "                **trace_groups[region_code],showlegend=(row_id==1),visible=show_regions),row_id,1)\n",
    "            row_id += 1\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        legend_title_text='<b>Region</b>',\n",
    "        template='plotly+gaa_template',\n",
    "        height=(row_height*rows) + 25)\n",
    "    fig.show()\n",
    "\n",
    "# create a series of scatter plot matrix that shows per rolling window correlations between:\n",
    "#   - available metrics contained in the analysis\n",
    "#   - time shifts contained in the analysis\n",
    "def create_splom(covid_pairs_df, analysis, window, color_discrete_map, show_regions = True):\n",
    "    # define a title for the overall plot\n",
    "    title = \"<b>SPLOM (Population Weighted) {} for Rolling Window {}D</b>: {} - {}\".\\\n",
    "        format(analysis['country_code'],window,analysis['time_window']['window_start'].strftime('%d-%m-%Y'),\\\n",
    "               analysis['time_window']['window_end'].strftime('%d-%m-%Y'))\n",
    "\n",
    "    # define what scatter plots will be in the matrix\n",
    "    splom_dimensions = analysis['available_metrics'].index.tolist()\n",
    "    splom_labels = analysis['available_metrics'].label.to_dict()\n",
    "\n",
    "    # Fix the predictor to the first specified in the model.\n",
    "    # TODO: Support use of multiple predictors\n",
    "    predictor = analysis['model']['predictor'][0]\n",
    "    predictor_column_name = data_avg_column(predictor,analysis['pairs_query']['layers'][predictor]['aggregation'])\n",
    "\n",
    "    for shift in analysis['model']['time_shifts']:\n",
    "        splom_dimensions.append(time_shift_column(predictor_column_name,window, analysis['model']['rolling_window_type'],shift))\n",
    "        splom_labels[time_shift_column(predictor_column_name,window,analysis['model']['rolling_window_type'],shift)] = str(shift) + 'D'\n",
    "\n",
    "    # create the splom\n",
    "    fig = px.scatter_matrix(covid_pairs_df,dimensions=splom_dimensions,labels=splom_labels,\n",
    "        color=\"iso3166-2_code\",\n",
    "        symbol=\"iso3166-2_code\",\n",
    "        hover_name=\"iso3166-2_code\",\n",
    "        color_discrete_map=color_discrete_map)\n",
    "    fig.update_traces(diagonal_visible=False, showupperhalf=False, showlowerhalf=True, visible=show_regions)\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        legend_title_text='<b>Region Code</b>',\n",
    "        template='plotly+gaa_template')\n",
    "    fig.show()\n",
    "    \n",
    "# We build a Grid of Choropleths.\n",
    "#  - Each row contains one metric type, with one sliding window\n",
    "#  - Each column contains one time shift\n",
    "\n",
    "# for visualization we map the 'significant' and 'significant_value' columns to 3 discrete values (-1, 0, 1)\n",
    "def set_region_category(significant, significant_value):\n",
    "    category = 0\n",
    "    if significant == \"Y\":\n",
    "        category = 1 if significant_value > 0 else -1 \n",
    "    return category\n",
    "\n",
    "def set_region_hovertext(significant, significant_value_display):\n",
    "    hovertext = \"N\"\n",
    "    if significant == \"Y\":\n",
    "        hovertext = 'Y ('+ str(round(significant_value_display,2)) + ')'\n",
    "    return hovertext\n",
    "\n",
    "# create the choropleth object use to visualize a map on the grid\n",
    "def get_choropleth_dict(geo_trace, geo_json, geo_choropleth_corr_dataset, shift_label):\n",
    "    # set category to a 3-value to control discrete three_color_scale\n",
    "    geo_choropleth_datum = geo_choropleth_corr_dataset['data']\n",
    "    analysis_model = geo_choropleth_corr_dataset['model']\n",
    "    \n",
    "    geo_choropleth_datum[shift_label + '_region_category'] = geo_choropleth_datum.\\\n",
    "        apply(lambda x: set_region_category(x[shift_label + '_significant'],x[shift_label + '_' + geo_choropleth_corr_dataset['significant_field']]),axis=1)\n",
    "    \n",
    "    geo_choropleth_datum[shift_label + '_hovertext'] = geo_choropleth_datum.\\\n",
    "        apply(lambda x: set_region_hovertext(x[shift_label + '_significant'],x[shift_label + '_' + geo_choropleth_corr_dataset['significant_field_display']]),axis=1)\n",
    "    \n",
    "    if(analysis_model == 'gam'):\n",
    "        legend_title = 'Assocation'\n",
    "    elif(analysis_model == 'spearman'):\n",
    "        legend_title = 'Correlation'\n",
    "        \n",
    "    return dict(\n",
    "            type = 'choropleth',\n",
    "            geojson=geo_json, featureidkey=\"properties.iso3166-2_code\", #JSON with Polygon & ID field to match the DF with color info\n",
    "            locations=geo_choropleth_datum.index,#geo_choropleth_datum['iso3166-2_code'], #DF & ID to match to JSON for coloring\n",
    "            z = geo_choropleth_datum[shift_label + '_region_category'], # Data to be color-coded\n",
    "            zmin=-1, zmax=1, # Force a scale from -1 to 0 to a discrete color mapping\n",
    "            colorscale = three_color_scale(),\n",
    "            showscale=True,\n",
    "            name=geo_trace,\n",
    "            text=geo_choropleth_datum[shift_label + '_hovertext'],\n",
    "            hoverinfo=\"text+location\",\n",
    "            colorbar=dict(\n",
    "                title=dict(text=legend_title, side = \"top\", font=(dict(size=12))),\n",
    "                outlinecolor='black', outlinewidth=1, y=0.90, xpad=10, ypad=50,\n",
    "                tickmode=\"array\",\n",
    "                tickvals=[1, 0, -1],\n",
    "                ticktext=[\"Yes (Positive)\",\"No\",\"Yes (Negative)\"],\n",
    "                ticks=\"outside\",\n",
    "                lenmode=\"pixels\", len=150,\n",
    "            )\n",
    "    )\n",
    "        \n",
    "def create_choropleth_plots(analysis_results_datasets, country_region_json, analysis, row_height = 200, column_width = 250):\n",
    "    if(analysis['model']['model'] == 'gam'):\n",
    "        for predictor in analysis['model']['independent_variables']:\n",
    "            title = \"<b>GAM on COVID-19 & {} {} </b>: {} - {}\".\\\n",
    "                format(analysis['country_code'],predictor,analysis['time_window']['window_start'].strftime('%d-%m-%Y'),\\\n",
    "                       analysis['time_window']['window_end'].strftime('%d-%m-%Y'))\n",
    "\n",
    "            create_choropleth_plots_for_predictor(title, analysis_results_datasets, country_region_json, analysis, predictor)\n",
    "    elif(analysis['model']['model'] == 'spearman'):\n",
    "        predictor = analysis['model']['predictor'][0]\n",
    "        # define a title for the overall plot\n",
    "        title = \"<b>Spearman Correlation on COVID-19 & {} {} </b>: {} - {}\".\\\n",
    "            format(analysis['country_code'],predictor,analysis['time_window']['window_start'].strftime('%d-%m-%Y'),\\\n",
    "                   analysis['time_window']['window_end'].strftime('%d-%m-%Y'))\n",
    "\n",
    "        create_choropleth_plots_for_predictor(title, analysis_results_datasets, country_region_json, analysis, predictor)\n",
    "\n",
    "def create_choropleth_plots_for_predictor(title, analysis_results_datasets, country_region_json, analysis, predictor, row_height = 200, column_width = 250):    \n",
    "    # Each of the figures will get a title\n",
    "    row_titles = []\n",
    "    subplot_titles = []\n",
    "\n",
    "    for metric_column_name, metric_row in analysis['available_metrics'].iterrows():\n",
    "        for window in analysis['model']['rolling_windows']:\n",
    "            result_key = analysis_result_key(metric_column_name,predictor,window)\n",
    "            row_titles.append(\"[Window {}D]\".format(str(window)))            \n",
    "            \n",
    "            for shift in analysis['model']['time_shifts']:\n",
    "                subplot_titles.append(\"{} ({}D)\".format(analysis_results_datasets[result_key]['label'],str(shift)))\n",
    "\n",
    "    # create the grid layout that will hold the various plots\n",
    "    columns = len(analysis['model']['time_shifts'])\n",
    "    rows = len(analysis['available_metrics'])*len(analysis['model']['rolling_windows'])\n",
    "\n",
    "    # determine the size of the grid\n",
    "    column_widths = [0.4] * columns\n",
    "    row_heights = [0.4] * rows\n",
    "\n",
    "    grid_width = column_width * columns\n",
    "    grid_height = row_height * rows\n",
    "    \n",
    "    # create the subplot figure\n",
    "    grid_specs = [[{\"type\": \"choropleth\"}] * columns] * rows\n",
    "    fig = make_subplots(rows=rows, cols=columns, column_widths=column_widths, row_heights=row_heights, \n",
    "                        specs=grid_specs,row_titles=row_titles, subplot_titles=subplot_titles, shared_xaxes=True, \n",
    "                        vertical_spacing=0.05)\n",
    "\n",
    "    # add the plots to the grid layout\n",
    "    row_id = 1\n",
    "    col_id = 1\n",
    "\n",
    "    for metric_column_name, metric_row in analysis['available_metrics'].iterrows():\n",
    "        for window in analysis['model']['rolling_windows']:\n",
    "            result_key = analysis_result_key(metric_column_name,predictor,window)\n",
    "            for shift_label in time_shift_labels(analysis['model']['time_shifts']):\n",
    "                fig.add_trace(get_choropleth_dict(result_key,country_region_json,analysis_results_datasets[result_key],shift_label), row=row_id, col=col_id)\n",
    "                col_id += 1\n",
    "\n",
    "            row_id += 1\n",
    "            col_id = 1\n",
    "\n",
    "    # show the grid\n",
    "    fig.update_geos(fitbounds=\"locations\", projection={'type':'mercator'}, visible=False)\n",
    "    fig.update_layout(width=grid_width, height=grid_height, title_text=title, template='plotly+gaa_template')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# parameters to control visualizations\n",
    "CASES_BY_POPULATION     = 1000000 # value used to show weighted by population expressed in cases by ...\n",
    "SHOW_REGIONS            = \"legendonly\" # whether traces in plots are shown by default or not can be True, False, \"legendonly\"\n",
    "ROW_HEIGHT_OVERVIEW     = 100 # value to control height of rows in Overview Regions plot\n",
    "ROW_HEIGHT_CHOROPLETH   = 200\n",
    "COLUMN_WIDTH_CHOROPLETH = 250\n",
    "\n",
    "# create a mapping between the iso3166-2_code and a color\n",
    "# this is to control that different plots use the same color for the same region\n",
    "REGION_COLOR_MAP = categorical_color_map(COVID_PAIRS_DF_TIME_SLICED['iso3166-2_code'].unique())\n",
    "\n",
    "# to get a understanding of the data create overview plots\n",
    "create_region_overview_plots(COVID_PAIRS_DF_TIME_SLICED, ANALYSIS,\n",
    "                                CASES_BY_POPULATION, REGION_COLOR_MAP, ROW_HEIGHT_OVERVIEW, SHOW_REGIONS)\n",
    "\n",
    "# to get an initial understanding of the possible correlation generate SPLOM(s)\n",
    "if(ANALYSIS['model']['model'] == 'spearman'):\n",
    "    for window in ANALYSIS['model']['rolling_windows']:\n",
    "        create_splom(COVID_PAIRS_DF_TIME_SLICED, ANALYSIS, window, REGION_COLOR_MAP, SHOW_REGIONS)\n",
    "    \n",
    "# visualize the outcome of the regional correlation generate Choropleths\n",
    "create_choropleth_plots(ANALYSIS_RESULTS_DATASETS, COUNTRY_REGION_JSON, ANALYSIS, ROW_HEIGHT_CHOROPLETH, COLUMN_WIDTH_CHOROPLETH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
